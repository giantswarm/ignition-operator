conf/10-docker.rules: |
  -w /usr/bin/docker -k docker
  -w /var/lib/docker -k docker
  -w /etc/docker -k docker
  -w /etc/systemd/system/docker.service.d/10-giantswarm-extra-args.conf -k docker
  -w /etc/systemd/system/docker.service.d/01-wait-docker.conf -k docker
  -w /usr/lib/systemd/system/docker.service -k docker
  -w /usr/lib/systemd/system/docker.socket -k docker
conf/calicoctl.cfg: |
  apiVersion: projectcalico.org/v3
  kind: CalicoAPIConfig
  metadata:
  spec:
    etcdEndpoints: https://EtcdDomain:1234
    etcdKeyFile: /etc/kubernetes/ssl/etcd/server-key.pem
    etcdCertFile: /etc/kubernetes/ssl/etcd/server-crt.pem
    etcdCACertFile: /etc/kubernetes/ssl/etcd/server-ca.pem
conf/crictl: |
  runtime-endpoint: unix:///var/run/dockershim/dockershim.sock
  image-endpoint: unix:///var/run/dockershim/dockershim.sock
  timeout: 10
  debug: false
conf/etcd-alias: |
  alias etcdctl="ETCDCTL_API=3 \
      ETCDCTL_ENDPOINTS=https://EtcdDomain:1234 \
      ETCDCTL_CACERT=/etc/kubernetes/ssl/etcd/client-ca.pem \
      ETCDCTL_CERT=/etc/kubernetes/ssl/etcd/client-crt.pem \
      ETCDCTL_KEY=/etc/kubernetes/ssl/etcd/client-key.pem \
      etcdctl"
conf/hardening.conf: |
  fs.inotify.max_user_watches = 16384
  # Default is 128, doubling for nodes with many pods
  # See https://github.com/giantswarm/giantswarm/issues/7711
  fs.inotify.max_user_instances = 8192
  kernel.kptr_restrict = 2
  kernel.sysrq = 0
  net.ipv4.conf.all.log_martians = 1
  net.ipv4.conf.all.send_redirects = 0
  net.ipv4.conf.default.accept_redirects = 0
  net.ipv4.conf.default.log_martians = 1
  net.ipv4.tcp_timestamps = 0
  net.ipv6.conf.all.accept_redirects = 0
  net.ipv6.conf.default.accept_redirects = 0
  # Increased mmapfs because some applications, like ES, need higher limit to store data properly
  vm.max_map_count = 262144
  # Ingress controller performance improvements
  # See https://github.com/kubernetes/ingress-nginx/issues/1939
  net.core.somaxconn=32768
  net.ipv4.ip_local_port_range=1024 65535
  net.ipv4.conf.all.rp_filter = 1
  net.ipv4.conf.all.arp_ignore = 1
  net.ipv4.conf.all.arp_announce = 2
conf/install-debug-tools: |
  #!/bin/bash
  set -eu

  mkdir -p /opt/bin

  # download calicoctl
  CALICOCTL_VERSION=v3.9.1
  wget https://github.com/projectcalico/calicoctl/releases/download/${CALICOCTL_VERSION}/calicoctl-linux-amd64
  mv calicoctl-linux-amd64 /opt/bin/calicoctl
  chmod +x /opt/bin/calicoctl

  # download crictl
  CRICTL_VERSION=v1.15.0
  wget https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz
  tar xvf crictl-${CRICTL_VERSION}-linux-amd64.tar.gz
  mv crictl /opt/bin/crictl
  chmod +x /opt/bin/crictl
  rm crictl-${CRICTL_VERSION}-linux-amd64.tar.gz
conf/ip_vs.conf: |
  ip_vs
  ip_vs_rr
  ip_vs_wrr
  ip_vs_sh
  nf_conntrack_ipv4
conf/k8s-addons: |
  #!/bin/bash

  export KUBECONFIG=/etc/kubernetes/kubeconfig/addons.yaml
  # kubectl 1.12.2
  KUBECTL=RegistryDomain/giantswarm/docker-kubectl:f5cae44c480bd797dc770dd5f62d40b74063c0d7

  /usr/bin/docker pull $KUBECTL

  # wait for healthy master
  while [ "$(/usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL get cs | grep Healthy | wc -l)" -ne "3" ]; do sleep 1 && echo 'Waiting for healthy k8s'; done

  # label namespaces (required for network egress policies)
  NAMESPACES="default kube-system"
  for namespace in ${NAMESPACES}
  do
      if ! /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL get namespaces -l name=${namespace} | grep ${namespace}; then
          while
              /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL label namespace ${namespace} name=${namespace}
              [ "$?" -ne "0" ]
          do
              echo "failed to label namespace ${namespace}, retrying in 5 sec"
              sleep 5s
          done
      fi
  done

  # apply Security bootstrap (RBAC and PSP)
  SECURITY_FILES=""
  SECURITY_FILES="${SECURITY_FILES} rbac_bindings.yaml"
  SECURITY_FILES="${SECURITY_FILES} rbac_roles.yaml"
  SECURITY_FILES="${SECURITY_FILES} psp_policies.yaml"
  SECURITY_FILES="${SECURITY_FILES} psp_roles.yaml"
  SECURITY_FILES="${SECURITY_FILES} psp_binding.yaml"

  for manifest in $SECURITY_FILES
  do
      while
          /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$manifest
          [ "$?" -ne "0" ]
      do
          echo "failed to apply /srv/$manifest, retrying in 5 sec"
          sleep 5s
      done
  done

  # check for other master and remove it
  THIS_MACHINE=$(cat /etc/hostname)
  for master in $(/usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL get nodes --no-headers=true --selector role=master | awk '{print $1}')
  do
      if [ "$master" != "$THIS_MACHINE" ]; then
          /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL delete node $master
      fi
  done

  # wait for etcd dns (return code 35 is bad certificate which is good enough here)
  while
      curl "https://EtcdDomain:1234" -k 2>/dev/null >/dev/null
      RET_CODE=$?
      [ "$RET_CODE" -ne "35" ]
  do
      echo "Waiting for etcd to be ready . . "
      sleep 3s
  done

  # create kube-proxy configmap
  while
      /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv $KUBECTL create configmap kube-proxy --from-file=kube-proxy.yaml=/srv/kube-proxy-config.yaml -o yaml --dry-run \
      | /usr/bin/docker run  -i --log-driver=none -a stdin -a stdout -a stderr -e KUBECONFIG=${KUBECONFIG} -v /etc/kubernetes:/etc/kubernetes --net=host --rm $KUBECTL apply -n kube-system -f -
      [ "$?" -ne "0" ]
  do
      echo "failed to configure kube-proxy from /srv/kube-proxy-config.yaml, retrying in 5 sec"
      sleep 5s
  done

  # install kube-proxy
  PROXY_MANIFESTS="kube-proxy-sa.yaml kube-proxy-ds.yaml"
  for manifest in $PROXY_MANIFESTS
  do
      while
          /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$manifest
          [ "$?" -ne "0" ]
      do
          echo "failed to apply /srv/$manifest, retrying in 5 sec"
          sleep 5s
      done
  done
  echo "kube-proxy successfully installed"

  # restart ds to apply config from configmap
  /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL delete pods -l k8s-app=kube-proxy -n kube-system

  # apply calico
  CALICO_FILE="calico-all.yaml"

  while
      /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$CALICO_FILE
      [ "$?" -ne "0" ]
  do
      echo "failed to apply /srv/$manifest, retrying in 5 sec"
      sleep 5s
  done

  # wait for healthy calico - we check for pods - desired vs ready
  while
      # result of this is 'eval [ "$DESIRED_POD_COUNT" -eq "$READY_POD_COUNT" ]'
      /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL -n kube-system  get ds calico-node 2>/dev/null >/dev/null
      RET_CODE_1=$?
      eval $(/usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL -n kube-system get ds calico-node | tail -1 | awk '{print "[ \"" $2"\" -eq \""$4"\" ] "}')
      RET_CODE_2=$?
      [ "$RET_CODE_1" -ne "0" ] || [ "$RET_CODE_2" -ne "0" ]
  do
      echo "Waiting for calico to be ready . . "
      sleep 3s
  done

  # apply default storage class
  if [ -f /srv/default-storage-class.yaml ]; then
      while
          /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/default-storage-class.yaml
          [ "$?" -ne "0" ]
      do
          echo "failed to apply /srv/default-storage-class.yaml, retrying in 5 sec"
          sleep 5s
      done
  else
      echo "no default storage class to apply"
  fi

  # apply priority classes:
  PRIORITY_CLASSES_FILE="priority_classes.yaml"

  while
      /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$PRIORITY_CLASSES_FILE
      [ "$?" -ne "0" ]
  do
      echo "failed to apply /srv/$PRIORITY_CLASSES_FILE, retrying in 5 sec"
      sleep 5s
  done

  # apply network policies:
  NETWORK_POLICIES_FILE="network_policies.yaml"
  NAMESPACES="kube-system giantswarm"
  for namespace in ${NAMESPACES}; do
      while
        /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$NETWORK_POLICIES_FILE -n $namespace
        [ "$?" -ne "0" ]
      do
        echo "failed to apply /srv/$NETWORK_POLICIES_FILE, retrying in 5 sec"
        sleep 5s
      done
  done

  # apply k8s addons
  MANIFESTS=""
  MANIFESTS="${MANIFESTS} ingress-controller-svc.yaml"
  for manifest in $MANIFESTS
  do
      while
          /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$manifest
          [ "$?" -ne "0" ]
      do
          echo "failed to apply /srv/$manifest, retrying in 5 sec"
          sleep 5s
      done
  done
  echo "Addons successfully installed"
conf/sshd_config: |
  # Use most defaults for sshd configuration.
  Subsystem sftp internal-sftp
  ClientAliveInterval 180
  UseDNS no
  UsePAM yes
  PrintLastLog no # handled by PAM
  PrintMotd no # handled by PAM
  # Non defaults (#100)
  ClientAliveCountMax 2
  PasswordAuthentication no
  TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem
  MaxAuthTries 5
  LoginGraceTime 60
  AllowTcpForwarding no
  AllowAgentForwarding no
conf/trusted-user-ca-keys.pem: |
  SSOPublicKey
conf/wait-for-domains: |
  #!/bin/bash
  domains="EtcdDomain APIDomain quay.io"

  for domain in $domains; do
  until nslookup $domain; do
      echo "Waiting for domain $domain to be available"
      sleep 5
  done

  echo "Successfully resolved domain $domain"
  done
config/kube-proxy.yaml: |
  apiVersion: kubeproxy.config.k8s.io/v1alpha1
  clientConnection:
    kubeconfig: /etc/kubernetes/config/proxy-kubeconfig.yaml
  kind: KubeProxyConfiguration
  mode: iptables
  resourceContainer: /kube-proxy
  clusterCIDR: CalicoSubnet/CalicoCIDR
  metricsBindAddress: 0.0.0.0:10249
config/kubelet-master.yaml.tmpl: |
  kind: KubeletConfiguration
  apiVersion: kubelet.config.k8s.io/v1beta1
  address: ${DEFAULT_IPV4}
  port: 10250
  healthzBindAddress: ${DEFAULT_IPV4}
  healthzPort: 10248
  clusterDNS:
    - K8sDNSIP
  clusterDomain: K8sDomain
  staticPodPath: /etc/kubernetes/manifests
  evictionSoft:
    memory.available: "500Mi"
  evictionHard:
    memory.available: "200Mi"
    imagefs.available: "15%"
  evictionSoftGracePeriod:
    memory.available: "5s"
  evictionMaxPodGracePeriod: 60
  kubeReserved:
    cpu: 350m
    memory: 1280Mi
    ephemeral-storage: 1024Mi
  kubeReservedCgroup: /kubereserved.slice
  runtimeCgroups: /kubereserved.slice
  systemReserved:
    cpu: 250m
    memory: 384Mi
  systemReservedCgroup: /system.slice
  authentication:
    anonymous:
      enabled: true # Defaults to false as of 1.10
    webhook:
      enabled: false # Deafults to true as of 1.10
  authorization:
    mode: AlwaysAllow # Deafults to webhook as of 1.10
  featureGates:
    TTLAfterFinished: true
config/kubelet-worker.yaml.tmpl: |
  kind: KubeletConfiguration
  apiVersion: kubelet.config.k8s.io/v1beta1
  address: ${DEFAULT_IPV4}
  port: 10250
  healthzBindAddress: ${DEFAULT_IPV4}
  healthzPort: 10248
  clusterDNS:
    - K8sDNSIP
  clusterDomain: K8sDomain
  evictionSoft:
    memory.available: "500Mi"
  evictionHard:
    memory.available: "200Mi"
    imagefs.available: "15%"
  evictionSoftGracePeriod:
    memory.available: "5s"
  evictionMaxPodGracePeriod: 60
  kubeReserved:
    cpu: 250m
    memory: 768Mi
    ephemeral-storage: 1024Mi
  kubeReservedCgroup: /kubereserved.slice
  runtimeCgroups: /kubereserved.slice
  systemReserved:
    cpu: 250m
    memory: 384Mi
  systemReservedCgroup: /system.slice
  authentication:
    anonymous:
      enabled: true # Defaults to false as of 1.10
    webhook:
      enabled: false # Deafults to true as of 1.10
  authorization:
    mode: AlwaysAllow # Deafults to webhook as of 1.10
  featureGates:
    TTLAfterFinished: true
config/scheduler.yaml: |
  kind: KubeSchedulerConfiguration
  algorithmSource:
    provider: DefaultProvider
  apiVersion: kubescheduler.config.k8s.io/v1alpha1
  clientConnection:
    kubeconfig: /etc/kubernetes/kubeconfig/scheduler.yaml
  failureDomains: kubernetes.io/hostname,failure-domain.beta.kubernetes.io/zone,failure-domain.beta.kubernetes.io/region
  hardPodAffinitySymmetricWeight: 1
k8s-resource/calico-all.yaml: |
  # CALICO HAS SEPARATE MANIFEST FOR AZURE
  # the azure manifest can be found in: https://github.com/giantswarm/azure-operator/blob/master/service/controller/vX/cloudconfig/template.go
  # where X is the version of azure operator
  #
  # Extra changes:
  #  - Added resource limits to calico-node and calico-kube-controller.
  #  - Added resource limits to install-cni.
  #  - Added 'priorityClassName: system-cluster-critical' to calico daemonset.
  #
  # Calico Version v3.9.1
  # https://docs.projectcalico.org/v3.9/release-notes/
  # This manifest includes the following component versions:
  #   calico/node:v3.9.1
  #   calico/cni:v3.9.1
  #   calico/kube-controllers:v3.9.1

  # This ConfigMap is used to configure a self-hosted Calico installation.
  kind: ConfigMap
  apiVersion: v1
  metadata:
    name: calico-config
    namespace: kube-system
  data:
    # Configure this with the location of your etcd cluster.
    etcd_endpoints: "https://EtcdDomain:1234"

    # If you're using TLS enabled etcd uncomment the following.
    # You must also populate the Secret below with these files.
    etcd_ca: "/calico-secrets/client-ca.pem"
    etcd_cert: "/calico-secrets/client-crt.pem"
    etcd_key: "/calico-secrets/client-key.pem"
    # Typha is disabled.
    typha_service_name: "none"
    # Configure the backend to use.
    calico_backend: "bird"

    # Configure the MTU to use
    veth_mtu: "CalicoMTU"

    # The CNI network configuration to install on each node.  The special
    # values in this config will be automatically populated.
    cni_network_config: |-
      {
        "name": "k8s-pod-network",
        "cniVersion": "0.3.1",
        "plugins": [
          {
            "type": "calico",
            "log_level": "info",
            "nodename": "__KUBERNETES_NODE_NAME__",
            "etcd_endpoints": "__ETCD_ENDPOINTS__",
            "etcd_key_file": "__ETCD_KEY_FILE__",
            "etcd_cert_file": "__ETCD_CERT_FILE__",
            "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
            "mtu": __CNI_MTU__,
            "ipam": {
                "type": "calico-ipam"
            },
            "policy": {
                "type": "k8s"
            },
            "kubernetes": {
                "kubeconfig": "__KUBECONFIG_FILEPATH__"
            }
          },
          {
            "type": "portmap",
            "snat": true,
            "capabilities": {"portMappings": true}
          }
        ]
      }
  ---
  # Source: calico/templates/kdd-crds.yaml
  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
     name: felixconfigurations.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: FelixConfiguration
      plural: felixconfigurations
      singular: felixconfiguration
  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: ipamblocks.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: IPAMBlock
      plural: ipamblocks
      singular: ipamblock

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: blockaffinities.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: BlockAffinity
      plural: blockaffinities
      singular: blockaffinity

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: ipamhandles.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: IPAMHandle
      plural: ipamhandles
      singular: ipamhandle

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: ipamconfigs.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: IPAMConfig
      plural: ipamconfigs
      singular: ipamconfig

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: bgppeers.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: BGPPeer
      plural: bgppeers
      singular: bgppeer

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: bgpconfigurations.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: BGPConfiguration
      plural: bgpconfigurations
      singular: bgpconfiguration

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: ippools.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: IPPool
      plural: ippools
      singular: ippool

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: hostendpoints.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: HostEndpoint
      plural: hostendpoints
      singular: hostendpoint

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: clusterinformations.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: ClusterInformation
      plural: clusterinformations
      singular: clusterinformation

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: globalnetworkpolicies.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: GlobalNetworkPolicy
      plural: globalnetworkpolicies
      singular: globalnetworkpolicy

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: globalnetworksets.crd.projectcalico.org
  spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
      kind: GlobalNetworkSet
      plural: globalnetworksets
      singular: globalnetworkset

  ---

  apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    name: networkpolicies.crd.projectcalico.org
  spec:
    scope: Namespaced
    group: crd.projectcalico.org
    version: v1
    names:
      kind: NetworkPolicy
      plural: networkpolicies
      singular: networkpolicy

  ---
  # This manifest installs the calico-node container, as well
  # as the CNI plugins and network config on
  # each master and worker node in a Kubernetes cluster.
  kind: DaemonSet
  apiVersion: apps/v1
  metadata:
    name: calico-node
    namespace: kube-system
    labels:
      giantswarm.io/service-type: managed
      k8s-app: calico-node
  spec:
    selector:
      matchLabels:
        k8s-app: calico-node
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1
    template:
      metadata:
        labels:
          k8s-app: calico-node
          giantswarm.io/service-type: managed
        annotations:
          # This, along with the CriticalAddonsOnly toleration below,
          # marks the pod as a critical add-on, ensuring it gets
          # priority scheduling and that its resources are reserved
          # if it ever gets evicted.
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        nodeSelector:
          kubernetes.io/os: linux
        hostNetwork: true
        tolerations:
          # Make sure calico-node gets scheduled on all nodes.
          - effect: NoSchedule
            operator: Exists
          # Mark the pod as a critical add-on for rescheduling.
          - key: CriticalAddonsOnly
            operator: Exists
          - effect: NoExecute
            operator: Exists
        serviceAccountName: calico-node
        # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
        # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
        terminationGracePeriodSeconds: 0
        priorityClassName: system-cluster-critical
        initContainers:
          # This container installs the CNI binaries
          # and CNI network config file on each node.
          - name: install-cni
            image: RegistryDomain/giantswarm/cni:v3.9.1
            command: ["/install-cni.sh"]
            env:
              # Name of the CNI config file to create.
              - name: CNI_CONF_NAME
                value: "10-calico.conflist"
              # The CNI network config to install on each node.
              - name: CNI_NETWORK_CONFIG
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: cni_network_config
              # The location of the Calico etcd cluster.
              - name: ETCD_ENDPOINTS
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_endpoints
              # CNI MTU Config variable
              - name: CNI_MTU
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: veth_mtu
              # Prevents the container from sleeping forever.
              - name: SLEEP
                value: "false"
            resources:
              requests:
                cpu: 50m
                memory: 100Mi
              limits:
                cpu: 50m
                memory: 100Mi
            volumeMounts:
              - mountPath: /host/opt/cni/bin
                name: cni-bin-dir
              - mountPath: /host/etc/cni/net.d
                name: cni-net-dir
              - mountPath: /calico-secrets
                name: etcd-certs
        containers:
          # Runs calico-node container on each Kubernetes node.  This
          # container programs network policy and routes on each
          # host.
          - name: calico-node
            image: RegistryDomain/giantswarm/node:v3.9.1
            env:
              # The location of the Calico etcd cluster.
              - name: ETCD_ENDPOINTS
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_endpoints
              # Location of the CA certificate for etcd.
              - name: ETCD_CA_CERT_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_ca
              # Location of the client key for etcd.
              - name: ETCD_KEY_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_key
              # Location of the client certificate for etcd.
              - name: ETCD_CERT_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_cert
              # Set noderef for node controller.
              - name: CALICO_K8S_NODE_REF
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
              # Choose the backend to use.
              - name: CALICO_NETWORKING_BACKEND
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: calico_backend
              # Cluster type to identify the deployment type
              - name: CLUSTER_TYPE
                value: "k8s,bgp"
              # Auto-detect the BGP IP address.
              - name: IP
                value: "autodetect"
              # Enable IPIP
              - name: CALICO_IPV4POOL_IPIP
                value: "Always"
              # Set MTU for tunnel device used if ipip is enabled
              - name: FELIX_IPINIPMTU
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: veth_mtu
              # The default IPv4 pool to create on startup if none exists. Pod IPs will be
              # chosen from this range. Changing this value after installation will have
              # no effect. This should fall within `--cluster-cidr`.
              - name: CALICO_IPV4POOL_CIDR
                value: "CalicoSubnet/CalicoCIDR"
              # Disable file logging so `kubectl logs` works.
              - name: CALICO_DISABLE_FILE_LOGGING
                value: "true"
              # Set Felix endpoint to host default action to ACCEPT.
              - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                value: "ACCEPT"
              # Disable IPv6 on Kubernetes.
              - name: FELIX_IPV6SUPPORT
                value: "false"
              # Set Felix logging to "info"
              - name: FELIX_LOGSEVERITYSCREEN
                value: "Warning"
              - name: FELIX_HEALTHENABLED
                value: "true"
              - name: FELIX_PROMETHEUSMETRICSENABLED
                value: "true"
            securityContext:
              privileged: true
            resources:
              requests:
                cpu: 250m
                memory: 150Mi
            livenessProbe:
              exec:
                command:
                - /bin/calico-node
                - -felix-live
              periodSeconds: 10
              initialDelaySeconds: 10
              failureThreshold: 6
            readinessProbe:
              exec:
                command:
                  - /bin/calico-node
                  - -felix-ready
                  - -bird-ready
              periodSeconds: 10
            volumeMounts:
              - mountPath: /lib/modules
                name: lib-modules
                readOnly: true
              - mountPath: /run/xtables.lock
                name: xtables-lock
                readOnly: false
              - mountPath: /var/run/calico
                name: var-run-calico
                readOnly: false
              - mountPath: /var/lib/calico
                name: var-lib-calico
                readOnly: false
              - mountPath: /calico-secrets
                name: etcd-certs
        volumes:
          # Used by calico-node.
          - name: lib-modules
            hostPath:
              path: /lib/modules
          - name: var-run-calico
            hostPath:
              path: /var/run/calico
          - name: var-lib-calico
            hostPath:
              path: /var/lib/calico
          - name: xtables-lock
            hostPath:
              path: /run/xtables.lock
              type: FileOrCreate
          # Used to install CNI.
          - name: cni-bin-dir
            hostPath:
              path: /opt/cni/bin
          - name: cni-net-dir
            hostPath:
              path: /etc/cni/net.d
          # Mount in the etcd TLS secrets.
          # See https://kubernetes.io/docs/concepts/configuration/secret/
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/ssl/etcd
  ---

  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: calico-node
    namespace: kube-system

  ---
  # This manifest deploys the Calico Kubernetes controllers.
  # See https://github.com/projectcalico/kube-controllers
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: calico-kube-controllers
    namespace: kube-system
    labels:
      giantswarm.io/service-type: managed
      k8s-app: calico-kube-controllers
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ''
  spec:
    # The controllers can only have a single active instance.
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
      type: Recreate
    template:
      metadata:
        name: calico-kube-controllers
        namespace: kube-system
        labels:
          k8s-app: calico-kube-controllers
          giantswarm.io/service-type: managed
      spec:
        nodeSelector:
          kubernetes.io/os: linux
          kubernetes.io/role: master
        # The controllers must run in the host network namespace so that
        # it isn't governed by policy that would prevent it from working.
        hostNetwork: true
        tolerations:
          # Mark the pod as a critical add-on for rescheduling.
          - key: CriticalAddonsOnly
            operator: Exists
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
        serviceAccountName: calico-kube-controllers
        priorityClassName: system-cluster-critical
        containers:
          - name: calico-kube-controllers
            image: RegistryDomain/giantswarm/kube-controllers:v3.9.1
            env:
              # The location of the Calico etcd cluster.
              - name: ETCD_ENDPOINTS
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_endpoints
              # Location of the CA certificate for etcd.
              - name: ETCD_CA_CERT_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_ca
              # Location of the client key for etcd.
              - name: ETCD_KEY_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_key
              # Location of the client certificate for etcd.
              - name: ETCD_CERT_FILE
                valueFrom:
                  configMapKeyRef:
                    name: calico-config
                    key: etcd_cert
              # Choose which controllers to run.
              - name: ENABLED_CONTROLLERS
                value: policy,profile,workloadendpoint,node,serviceaccount
            volumeMounts:
              # Mount in the etcd TLS secrets.
              - mountPath: /calico-secrets
                name: etcd-certs
            resources:
              requests:
                cpu: 250m
                memory: 100Mi
              limits:
                cpu: 250m
                memory: 100Mi
            readinessProbe:
              exec:
                command:
                  - /usr/bin/check-status
                  - -r
        volumes:
          # Mount in the etcd TLS secrets with mode 400.
          # See https://kubernetes.io/docs/concepts/configuration/secret/
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/ssl/etcd

  ---

  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: calico-kube-controllers
    namespace: kube-system
  ---
  ---
  # Include a clusterrole for the kube-controllers component,
  # and bind it to the calico-kube-controllers serviceaccount.
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: calico-kube-controllers
  rules:
  - apiGroups:
    - ""
    resources:
    - nodes
    - pods
    - namespaces
    - serviceaccounts
    verbs:
    - watch
    - list
    - get
  - apiGroups:
    - networking.k8s.io
    resources:
    - networkpolicies
    verbs:
    - watch
    - list
    # IPAM resources are manipulated when nodes are deleted.
  - apiGroups:
    - crd.projectcalico.org
    resources:
    - ippools
    verbs:
    - list
  - apiGroups:
    - crd.projectcalico.org
    resources:
    - blockaffinities
    - ipamblocks
    - ipamhandles
    verbs:
    - get
    - list
    - create
    - update
    - delete
  - apiGroups:
    - crd.projectcalico.org
    resources:
    - clusterinformations
    verbs:
    - get
    - create
    - update
  ---
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: calico-kube-controllers
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-kube-controllers
  subjects:
    - kind: ServiceAccount
      name: calico-kube-controllers
      namespace: kube-system
  ---
  # Include a clusterrole for the calico-node DaemonSet,
  # and bind it to the calico-node serviceaccount.
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: calico-node
  rules:
    # The CNI plugin needs to get pods, nodes, and namespaces.
    - apiGroups: [""]
      resources:
        - pods
        - nodes
        - namespaces
      verbs:
        - get
    - apiGroups: [""]
      resources:
        - endpoints
        - services
      verbs:
        # Used to discover service IPs for advertisement.
        - watch
        - list
        # Used to discover Typhas.
        - get
    - apiGroups: [""]
      resources:
        - nodes/status
      verbs:
        # Needed for clearing NodeNetworkUnavailable flag.
        - patch
        # Calico stores some configuration information in node annotations.
        - update
    # Watch for changes to Kubernetes NetworkPolicies.
    - apiGroups: ["networking.k8s.io"]
      resources:
        - networkpolicies
      verbs:
        - watch
        - list
    # Used by Calico for policy information.
    - apiGroups: [""]
      resources:
        - pods
        - namespaces
        - serviceaccounts
      verbs:
        - list
        - watch
    # The CNI plugin patches pods/status.
    - apiGroups: [""]
      resources:
        - pods/status
      verbs:
        - patch
    # Calico monitors various CRDs for config.
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - globalfelixconfigs
        - felixconfigurations
        - bgppeers
        - globalbgpconfigs
        - bgpconfigurations
        - ippools
        - ipamblocks
        - globalnetworkpolicies
        - globalnetworksets
        - networkpolicies
        - networksets
        - clusterinformations
        - hostendpoints
        - blockaffinities
      verbs:
        - get
        - list
        - watch
    # Calico must create and update some CRDs on startup.
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - ippools
        - felixconfigurations
        - clusterinformations
      verbs:
        - create
        - update
    # Calico stores some configuration information on the node.
    - apiGroups: [""]
      resources:
        - nodes
      verbs:
        - get
        - list
        - watch
    # These permissions are only requried for upgrade from v2.6, and can
    # be removed after upgrade or on fresh installations.
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - bgpconfigurations
        - bgppeers
      verbs:
        - create
        - update
    # These permissions are required for Calico CNI to perform IPAM allocations.
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - blockaffinities
        - ipamblocks
        - ipamhandles
      verbs:
        - get
        - list
        - create
        - update
        - delete
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - ipamconfigs
      verbs:
        - get
    # Block affinities must also be watchable by confd for route aggregation.
    - apiGroups: ["crd.projectcalico.org"]
      resources:
        - blockaffinities
      verbs:
        - watch
    # The Calico IPAM migration needs to get daemonsets. These permissions can be
    # removed if not upgrading from an installation using host-local IPAM.
    - apiGroups: ["apps"]
      resources:
        - daemonsets
      verbs:
        - get
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: calico-node
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-node
  subjects:
    - kind: ServiceAccount
      name: calico-node
      namespace: kube-system
k8s-resource/ingress-controller-svc.yaml: |
  apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    name: nginx-ingress-controller
    namespace: kube-system
    labels:
      k8s-app: nginx-ingress-controller
  spec:
    type: NodePort
    ports:
    - name: http
      port: 80
      nodePort: 30010
      protocol: TCP
      targetPort: 80
    - name: https
      port: 443
      nodePort: 30011
      protocol: TCP
      targetPort: 443
    selector:
      k8s-app: nginx-ingress-controller
k8s-resource/k8s-encryption-config.yaml: |
  kind: EncryptionConfiguration
  apiVersion: apiserver.config.k8s.io/v1
  resources:
    - resources:
      - secrets
      providers:
      - aescbc:
          keys:
          - name: key1
            secret: some secret
      - identity: {}
k8s-resource/kube-proxy-ds.yaml: |
  kind: DaemonSet
  apiVersion: extensions/v1beta1
  metadata:
    name: kube-proxy
    namespace: kube-system
    labels:
      component: kube-proxy
      k8s-app: kube-proxy
      kubernetes.io/cluster-service: "true"
  spec:
    selector:
      matchLabels:
        k8s-app: kube-proxy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1
    template:
      metadata:
        labels:
          component: kube-proxy
          k8s-app: kube-proxy
          kubernetes.io/cluster-service: "true"
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        # Make sure the pod gets scheduled on all nodes.
        - operator: Exists
        hostNetwork: true
        priorityClassName: system-node-critical
        serviceAccountName: kube-proxy
        containers:
          - name: kube-proxy
            image: RegistryDomain/K8sImage
            command:
            - /hyperkube
            - kube-proxy
            - --config=/etc/kubernetes/config/proxy-config.yml
            - --v=2
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10256
              initialDelaySeconds: 10
              periodSeconds: 3
            resources:
              requests:
                memory: "80Mi"
                cpu: "75m"
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /etc/kubernetes/config/
              name: k8s-config
            - mountPath: /etc/kubernetes/kubeconfig/
              name: k8s-kubeconfig
              readOnly: true
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/config/
          name: k8s-config
        - hostPath:
            path: /etc/kubernetes/config/
          name: k8s-kubeconfig
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - hostPath:
            path: /lib/modules
          name: lib-modules
k8s-resource/kube-proxy-sa.yaml: |-
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: kube-proxy
    namespace: kube-system
k8s-resource/network_policies.yaml: |
  ---
  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: default-deny-all
  spec:
    podSelector: {}
    policyTypes:
    - Ingress
    - Egress
k8s-resource/priority_classes.yaml: |
  apiVersion: scheduling.k8s.io/v1
  kind: PriorityClass
  metadata:
    name: giantswarm-critical
  value: 1000000000
  globalDefault: false
  description: "This priority class is used by giantswarm kubernetes components."
k8s-resource/psp_bindings.yaml: |
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
      name: privileged-psp-users
  subjects:
  - kind: ServiceAccount
    name: calico-node
    namespace: kube-system
  - kind: ServiceAccount
    name: calico-kube-controllers
    namespace: kube-system
  - kind: ServiceAccount
    name: kube-proxy
    namespace: kube-system
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: privileged-psp-user
  ---
  # grants the restricted PSP role to
  # the all authenticated users.
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
      name: restricted-psp-users
  subjects:
  - kind: Group
    apiGroup: rbac.authorization.k8s.io
    name: system:authenticated
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: restricted-psp-user
k8s-resource/psp_policies.yaml: |
  apiVersion: extensions/v1beta1
  kind: PodSecurityPolicy
  metadata:
    name: privileged
    annotations:
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
  spec:
    allowPrivilegeEscalation: true
    allowedCapabilities:
    - '*'
    fsGroup:
      rule: RunAsAny
    privileged: true
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
    - '*'
    hostPID: true
    hostIPC: true
    hostNetwork: true
    hostPorts:
    - min: 0
      max: 65536
  ---
  ---
  apiVersion: extensions/v1beta1
  kind: PodSecurityPolicy
  metadata:
    name: restricted
  spec:
    privileged: false
    allowPrivilegeEscalation: false
    runAsUser:
      ranges:
        - max: 65535
          min: 1000
      rule: MustRunAs
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: 'MustRunAs'
      ranges:
        - min: 1
          max: 65535
    fsGroup:
      rule: 'MustRunAs'
      ranges:
        - min: 1
          max: 65535
    volumes:
    - 'secret'
    - 'configMap'
    hostPID: false
    hostIPC: false
    hostNetwork: false
    readOnlyRootFilesystem: false
k8s-resource/psp_roles.yaml: |
  # restrictedPSP grants access to use
  # the restricted PSP.
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: restricted-psp-user
  rules:
  - apiGroups:
    - extensions
    resources:
    - podsecuritypolicies
    resourceNames:
    - restricted
    verbs:
    - use
  ---
  # privilegedPSP grants access to use the privileged
  # PSP.
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: privileged-psp-user
  rules:
  - apiGroups:
    - extensions
    resources:
    - podsecuritypolicies
    resourceNames:
    - privileged
    verbs:
    - use
k8s-resource/rbac_bindings.yaml: |
  ## User
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: giantswarm-admin
  subjects:
  - kind: User
    name: APIDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  ---
  ## Worker
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: kubelet
  subjects:
  - kind: User
    name: K8sKubeletDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: system:node
    apiGroup: rbac.authorization.k8s.io
  ---
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: proxy
  subjects:
  - kind: User
    name: K8sKubeletDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: system:node-proxier
    apiGroup: rbac.authorization.k8s.io
  ---
  ## Master
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: kube-controller-manager
  subjects:
  - kind: User
    name: APIDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: system:kube-controller-manager
    apiGroup: rbac.authorization.k8s.io
  ---
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: kube-scheduler
  subjects:
  - kind: User
    name: APIDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: system:kube-scheduler
    apiGroup: rbac.authorization.k8s.io
  ---
  ## node-operator
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: node-operator
  subjects:
  - kind: User
    name: node-operator.ClusterBaseDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: node-operator
    apiGroup: rbac.authorization.k8s.io
  ---
  ## prometheus-external is prometheus from host cluster
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: prometheus-external
  subjects:
  - kind: User
    name: prometheus.ClusterBaseDomain
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: prometheus-external
    apiGroup: rbac.authorization.k8s.io
k8s-resource/rbac_roles.yaml: |
  ## node-operator
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: node-operator
  rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["patch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["list", "delete"]
  ---
  ## prometheus-external
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: prometheus-external
  rules:
  - apiGroups: [""]
    resources:
    - nodes
    - nodes/proxy
    - services
    - endpoints
    - pods
    verbs: ["get", "list", "watch"]
  - apiGroups:
    - extensions
    resources:
    - ingresses
    verbs: ["get", "list", "watch"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
kubeconfig/addons.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: proxy
    user:
      client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
      client-key: /etc/kubernetes/ssl/apiserver-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: proxy
    name: service-account-context
  current-context: service-account-context
kubeconfig/controller-manager.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: controller-manager
    user:
      client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
      client-key: /etc/kubernetes/ssl/apiserver-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: controller-manager
    name: service-account-context
  current-context: service-account-context
kubeconfig/kube-proxy-master.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: proxy
    user:
      client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
      client-key: /etc/kubernetes/ssl/apiserver-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: proxy
    name: service-account-context
  current-context: service-account-context
kubeconfig/kube-proxy-worker.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: proxy
    user:
      client-certificate: /etc/kubernetes/ssl/worker-crt.pem
      client-key: /etc/kubernetes/ssl/worker-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/worker-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: proxy
    name: service-account-context
  current-context: service-account-context
kubeconfig/kubelet-master.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: kubelet
    user:
      client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
      client-key: /etc/kubernetes/ssl/apiserver-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: kubelet
    name: service-account-context
  current-context: service-account-context
kubeconfig/kubelet-worker.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: kubelet
    user:
      client-certificate: /etc/kubernetes/ssl/worker-crt.pem
      client-key: /etc/kubernetes/ssl/worker-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/worker-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: kubelet
    name: service-account-context
  current-context: service-account-context
kubeconfig/scheduler.yaml: |
  apiVersion: v1
  kind: Config
  users:
  - name: scheduler
    user:
      client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
      client-key: /etc/kubernetes/ssl/apiserver-key.pem
  clusters:
  - name: local
    cluster:
      certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
      server: https://APIDomain
  contexts:
  - context:
      cluster: local
      user: scheduler
    name: service-account-context
  current-context: service-account-context
manifests/k8s-api-healthz.yaml: |
  apiVersion: v1
  kind: Pod
  metadata:
    name: k8s-api-healthz
    namespace: kube-system
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ''
  spec:
    hostNetwork: true
    priorityClassName: system-node-critical
    containers:
      - name: k8s-api-healthz
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        command:
          - /k8s-api-healthz
          - --api-endpoint="https://$(HOST_IP):443/healthz"
        image: quay.io/giantswarm/k8s-api-healthz:1c0cdf1ed5ee18fdf59063ecdd84bf3787f80fac
        resources:
          requests:
            cpu: 50m
            memory: 20Mi
        volumeMounts:
        - mountPath: /etc/kubernetes/ssl/
          name: ssl-certs-kubernetes
          readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
manifests/k8s-api-server.yaml: |
  apiVersion: v1
  kind: Pod
  metadata:
    name: k8s-api-server
    namespace: kube-system
    labels:
      k8s-app: api-server
      tier: control-plane
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ''
  spec:
    hostNetwork: true
    priorityClassName: system-node-critical
    containers:
    - name: k8s-api-server
      image: RegistryDomain/K8sImage
      env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      command:
      - /hyperkube
      - kube-apiserver
      - --allow-privileged=true
      - --anonymous-auth=false
      - --insecure-port=0
      - --kubelet-https=true
      - --kubelet-preferred-address-types=InternalIP
      - --secure-port=9001
      - --bind-address=0.0.0.0
      - --etcd-prefix=EtcdPrefix
      - --profiling=false
      - --service-account-lookup=true
      - --authorization-mode=RBAC
      - --feature-gates=TTLAfterFinished=true
      - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,DefaultStorageClass,PersistentVolumeClaimResize,PodSecurityPolicy,Priority,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
      - --cloud-provider=cloudProvider
      - --service-cluster-ip-range=K8sIPRange
      - --etcd-servers=https://127.0.0.1:2379
      - --etcd-cafile=/etc/kubernetes/ssl/etcd/server-ca.pem
      - --etcd-certfile=/etc/kubernetes/ssl/etcd/server-crt.pem
      - --etcd-keyfile=/etc/kubernetes/ssl/etcd/server-key.pem
      - --advertise-address=$(HOST_IP)
      - --runtime-config=api/all=true,scheduling.k8s.io/v1alpha1=true
      - --logtostderr=true
      - --tls-cert-file=/etc/kubernetes/ssl/apiserver-crt.pem
      - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --client-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem
      - --service-account-key-file=/etc/kubernetes/ssl/service-account-key.pem
      - --audit-log-path=/var/log/apiserver/audit.log
      - --audit-log-maxage=30
      - --audit-log-maxbackup=30
      - --audit-log-maxsize=100
      - --audit-policy-file=/etc/kubernetes/policies/audit-policy.yaml
      - --encryption-provider-config=/etc/kubernetes/encryption/k8s-encryption-config.yaml
      - --requestheader-client-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem
      - --requestheader-allowed-names=aggregator,APIDomain,K8sKubeletDomain
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --proxy-client-cert-file=/etc/kubernetes/ssl/apiserver-crt.pem
      - --proxy-client-key-file=/etc/kubernetes/ssl/apiserver-key.pem
      - --oidc-issuer-url=issuerURL
      - --oidc-client-id=clientID
      - --oidc-username-claim=username-claim
      - --oidc-username-prefix=username-prefix
      - --oidc-groups-claim=groups-claim
      - --oidc-groups-prefix=groups-prefix
      resources:
        requests:
          cpu: 300m
          memory: 300Mi
      livenessProbe:
        tcpSocket:
          port: 9001
        initialDelaySeconds: 15
        timeoutSeconds: 15
      ports:
      - containerPort: 9001
        hostPort: 9001
        name: https
      volumeMounts:
      - mountPath: /var/log/apiserver/
        name: apiserver-log
      - mountPath: /etc/kubernetes/encryption/
        name: k8s-encryption
        readOnly: true
      - mountPath: /etc/kubernetes/manifests
        name: k8s-manifests
        readOnly: true
      - mountPath: /etc/kubernetes/policies
        name: k8s-policies
        readOnly: true
      - mountPath: /etc/kubernetes/secrets/
        name: k8s-secrets
        readOnly: true
      - mountPath: /etc/kubernetes/ssl/
        name: ssl-certs-kubernetes
        readOnly: true
    volumes:
    - hostPath:
        path: /var/log/apiserver/
      name: apiserver-log
    - hostPath:
        path: /etc/kubernetes/encryption/
      name: k8s-encryption
    - hostPath:
        path: /etc/kubernetes/manifests
      name: k8s-manifests
    - hostPath:
        path: /etc/kubernetes/policies
      name: k8s-policies
    - hostPath:
        path: /etc/kubernetes/secrets
      name: k8s-secrets
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
manifests/k8s-controller-manager.yaml: |
  apiVersion: v1
  kind: Pod
  metadata:
    name: k8s-controller-manager
    namespace: kube-system
    labels:
      k8s-app: controller-manager
      tier: control-plane
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ''
  spec:
    hostNetwork: true
    priorityClassName: system-node-critical
    containers:
    - name: k8s-controller-manager
      image: RegistryDomain/K8sImage
      command:
      - /hyperkube
      - kube-controller-manager
      - --logtostderr=true
      - --v=2
      - --cloud-provider=aws
      - --terminated-pod-gc-threshold=10
      - --use-service-account-credentials=true
      - --kubeconfig=/etc/kubernetes/kubeconfig/controller-manager.yaml
      - --feature-gates=TTLAfterFinished=true
      - --root-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem
      - --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem
      resources:
        requests:
          cpu: 200m
          memory: 200Mi
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
        initialDelaySeconds: 15
        timeoutSeconds: 15
      volumeMounts:
      - mountPath: /etc/kubernetes/config/
        name: k8s-config
        readOnly: true
      - mountPath: /etc/kubernetes/kubeconfig/
        name: k8s-kubeconfig
        readOnly: true
      - mountPath: /etc/kubernetes/secrets/
        name: k8s-secrets
        readOnly: true
      - mountPath: /etc/kubernetes/ssl/
        name: ssl-certs-kubernetes
        readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/config
      name: k8s-config
    - hostPath:
        path: /etc/kubernetes/kubeconfig
      name: k8s-kubeconfig
    - hostPath:
        path: /etc/kubernetes/secrets
      name: k8s-secrets
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
manifests/k8s-scheduler.yaml: |
  apiVersion: v1
  kind: Pod
  metadata:
    name: k8s-scheduler
    namespace: kube-system
    labels:
      k8s-app: scheduler
      tier: control-plane
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ''
  spec:
    hostNetwork: true
    priorityClassName: system-node-critical
    containers:
    - name: k8s-scheduler
      image: RegistryDomain/K8sImage
      command:
      - /hyperkube
      - kube-scheduler
      - --feature-gates=TTLAfterFinished=true
      - --config=/etc/kubernetes/config/scheduler.yaml
      - --v=2
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
        initialDelaySeconds: 15
        timeoutSeconds: 15
      volumeMounts:
      - mountPath: /etc/kubernetes/config/
        name: k8s-config
        readOnly: true
      - mountPath: /etc/kubernetes/kubeconfig/
        name: k8s-kubeconfig
        readOnly: true
      - mountPath: /etc/kubernetes/ssl/
        name: ssl-certs-kubernetes
        readOnly: true
    volumes:
    - hostPath:
        path: /etc/kubernetes/config
      name: k8s-config
    - hostPath:
        path: /etc/kubernetes/kubeconfig
      name: k8s-kubeconfig
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
policies/audit-policy.yaml: |
  apiVersion: audit.k8s.io/v1
  kind: Policy
  rules:
    # The following requests were manually identified as high-volume and low-risk,
    # so drop them.
    - level: None
      users: ["system:kube-proxy"]
      verbs: ["watch"]
      resources:
        - group: "" # core
          resources: ["endpoints", "services", "services/status"]
    - level: None
      # Ingress controller reads 'configmaps/ingress-uid' through the unsecured port.
      users: ["system:unsecured"]
      namespaces: ["kube-system"]
      verbs: ["get"]
      resources:
        - group: "" # core
          resources: ["configmaps"]
    - level: None
      users: ["kubelet"] # legacy kubelet identity
      verbs: ["get"]
      resources:
        - group: "" # core
          resources: ["nodes", "nodes/status"]
    - level: None
      userGroups: ["system:nodes"]
      verbs: ["get"]
      resources:
        - group: "" # core
          resources: ["nodes", "nodes/status"]
    - level: None
      users:
        - system:kube-controller-manager
        - system:kube-scheduler
        - system:serviceaccount:kube-system:endpoint-controller
      verbs: ["get", "update"]
      namespaces: ["kube-system"]
      resources:
        - group: "" # core
          resources: ["endpoints"]
    - level: None
      users: ["system:apiserver"]
      verbs: ["get"]
      resources:
        - group: "" # core
          resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
    - level: None
      users: ["system:serviceaccount:kube-system:cluster-autoscaler"]
      verbs: ["get", "update"]
      namespaces: ["kube-system"]
      resources:
        - group: "" # core
          resources: ["configmaps", "endpoints"]
    # Don't log HPA fetching metrics.
    - level: None
      users:
        - system:kube-controller-manager
      verbs: ["get", "list"]
      resources:
        - group: "metrics.k8s.io"
    # Don't log these read-only URLs.
    - level: None
      nonResourceURLs:
        - /healthz*
        - /version
        - /swagger*
    # Don't log events requests.
    - level: None
      resources:
        - group: "" # core
          resources: ["events"]
    # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes
    - level: Request
      users:
        [
          "kubelet",
          "system:node-problem-detector",
          "system:serviceaccount:kube-system:node-problem-detector",
        ]
      verbs: ["update", "patch"]
      resources:
        - group: "" # core
          resources: ["nodes/status", "pods/status"]
      omitStages:
        - "RequestReceived"
    - level: Request
      userGroups: ["system:nodes"]
      verbs: ["update", "patch"]
      resources:
        - group: "" # core
          resources: ["nodes/status", "pods/status"]
      omitStages:
        - "RequestReceived"
    # deletecollection calls can be large, don't log responses for expected namespace deletions
    - level: Request
      users: ["system:serviceaccount:kube-system:namespace-controller"]
      verbs: ["deletecollection"]
      omitStages:
        - "RequestReceived"
    # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
    # so only log at the Metadata level.
    - level: Metadata
      resources:
        - group: "" # core
          resources: ["secrets", "configmaps"]
        - group: authentication.k8s.io
          resources: ["tokenreviews"]
      omitStages:
        - "RequestReceived"
    # Get repsonses can be large; skip them.
    - level: Request
      verbs: ["get", "list", "watch"]
      resources:
        - group: "" # core
        - group: "admissionregistration.k8s.io"
        - group: "apiextensions.k8s.io"
        - group: "apiregistration.k8s.io"
        - group: "apps"
        - group: "authentication.k8s.io"
        - group: "authorization.k8s.io"
        - group: "autoscaling"
        - group: "batch"
        - group: "certificates.k8s.io"
        - group: "extensions"
        - group: "metrics.k8s.io"
        - group: "networking.k8s.io"
        - group: "policy"
        - group: "rbac.authorization.k8s.io"
        - group: "scheduling.k8s.io"
        - group: "settings.k8s.io"
        - group: "storage.k8s.io"
      omitStages:
        - "RequestReceived"
    # Default level for known APIs
    - level: RequestResponse
      resources:
        - group: "" # core
        - group: "admissionregistration.k8s.io"
        - group: "apiextensions.k8s.io"
        - group: "apiregistration.k8s.io"
        - group: "apps"
        - group: "authentication.k8s.io"
        - group: "authorization.k8s.io"
        - group: "autoscaling"
        - group: "batch"
        - group: "certificates.k8s.io"
        - group: "extensions"
        - group: "metrics.k8s.io"
        - group: "networking.k8s.io"
        - group: "policy"
        - group: "rbac.authorization.k8s.io"
        - group: "scheduling.k8s.io"
        - group: "settings.k8s.io"
        - group: "storage.k8s.io"
      omitStages:
        - "RequestReceived"
    # Default level for all other requests.
    - level: Metadata
      omitStages:
        - "RequestReceived"
