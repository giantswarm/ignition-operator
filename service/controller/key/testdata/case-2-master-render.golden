.: "ignition:\n  version: \"2.2.0\"\npasswd:\n  users:\n    - name: giantswarm\n      shell:
  \"/bin/bash\"\n      uid: 1000\n      groups:\n        - \"sudo\"\n        - \"docker\"\n
  \     sshAuthorizedKeys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCuJvxy3FKGrfJ4XB5exEdKXiqqteXEPFzPtex6dC0lHyigtO7l+NXXbs9Lga2+Ifs0Tza92MRhg/FJ+6za3oULFo7+gDyt86DIkZkMFdnSv9+YxYe+g4zqakSV+bLVf2KP6krUGJb7t4Nb+gGH62AiUx+58Onxn5rvYC0/AXOYhkAiH8PydXTDJDPhSA/qWSWEeCQistpZEDFnaVi0e7uq/k3hWJ+v9Gz0qqChHKWWOYp3W6aiIE3G6gLOXNEBdWRrjK6xmrSmo9Toqh1G7iIV0Y6o9w5gIHJxf6+8X70DCuVDx9OLHmjjMyGnd+1c3yTFMUdugtvmeiGWE0E7ZjNSNIqWlnvYJ0E1XPBiyQ7nhitOtVvPC4kpRP7nOFiCK9n8Lr3z3p4v3GO0FU3/qvLX+ECOrYK316gtwSJMd+HIouCbaJaFGvT34peaq1uluOP/JE+rFOnszZFpCYgTY2b4lWjf2krkI/a/3NDJPnRpjoE3RjmbepkZeIdOKTCTH1xYZ3O8dWKRX8X4xORvKJO+oV2UdoZlFa/WJTmq23z4pCVm0UWDYR5C2b9fHwxh/xrPT7CQ0E+E9wmeOvR4wppDMseGQCL+rSzy2AYiQ3D8iQxk0r6T+9MyiRCfuY73p63gB3m37jMQSLHvm77MkRnYcBy61Qxk+y+ls2D0xJfqxw==
  giantswarm\"\n\nsystemd:\n  units:\n  # Start - manual management for cgroup structure\n
  \ - name: kubereserved.slice\n    path: /etc/systemd/system/kubereserved.slice\n
  \   content: |\n            [Unit]\n      Description=Limited resources slice for
  Kubernetes services\n      Documentation=man:systemd.special(7)\n      DefaultDependencies=no\n
  \     Before=slices.target\n      Requires=-.slice\n      After=-.slice\n      \n
  \ # End - manual management for cgroup structure\n  - name: audit-rules.service\n
  \   enabled: true\n    dropins:\n    - name: 10-Wait-For-Docker.conf\n      contents:
  |\n                  [Service]\n          ExecStartPre=/bin/bash -c &#34;while [
  ! -f /etc/audit/rules.d/10-docker.rules ]; do echo &#39;Waiting for /etc/audit/rules.d/10-docker.rules
  to be written&#39; &amp;&amp; sleep 1; done&#34;\n          \n  - name: set-certs-group-owner-permission-giantswarm.service\n
  \   enabled: true\n    contents: |\n            [Unit]\n      Description=Change
  group owner for certificates to giantswarm\n      Wants=k8s-kubelet.service k8s-setup-network-env.service\n
  \     After=k8s-kubelet.service k8s-setup-network-env.service\n      [Service]\n
  \     Type=oneshot\n      ExecStart=/bin/sh -c &#34;find /etc/kubernetes/ssl -name
  &#39;*.pem&#39; -print | xargs -i  sh -c &#39;chown root:giantswarm {} &amp;&amp;
  chmod 640 {}&#39;&#34;\n      [Install]\n      WantedBy=multi-user.target\n      \n
  \ - name: wait-for-domains.service\n    enabled: true\n    contents: |\n            [Unit]\n
  \     Description=Wait for etcd and k8s API domains to be available\n      [Service]\n
  \     Type=oneshot\n      ExecStart=/opt/wait-for-domains\n      [Install]\n      WantedBy=multi-user.target\n
  \     \n  - name: os-hardeing.service\n    enabled: true\n    contents: |\n            [Unit]\n
  \     Description=Apply os hardening\n      [Service]\n      Type=oneshot\n      ExecStartPre=-/bin/bash
  -c &#34;gpasswd -d core rkt; gpasswd -d core docker; gpasswd -d core wheel&#34;\n
  \     ExecStartPre=/bin/bash -c &#34;until [ -f &#39;/etc/sysctl.d/hardening.conf&#39;
  ]; do echo Waiting for sysctl file; sleep 1s;done;&#34;\n      ExecStart=/usr/sbin/sysctl
  -p /etc/sysctl.d/hardening.conf\n      [Install]\n      WantedBy=multi-user.target\n
  \     \n  - name: k8s-setup-kubelet-config.service\n    enabled: true\n    contents:
  |\n            [Unit]\n      Description=k8s-setup-kubelet-config Service\n      After=k8s-setup-network-env.service
  docker.service\n      Requires=k8s-setup-network-env.service docker.service\n      [Service]\n
  \     Type=oneshot\n      RemainAfterExit=yes\n      TimeoutStartSec=0\n      EnvironmentFile=/etc/network-environment\n
  \     ExecStart=/bin/bash -c &#39;/usr/bin/envsubst &lt;/etc/kubernetes/config/kubelet.yaml.tmpl
  &gt;/etc/kubernetes/config/kubelet.yaml&#39;\n      [Install]\n      WantedBy=multi-user.target\n
  \     \n  - name: containerd.service\n    enabled: true\n    contents: |\n    dropins:\n
  \     - name: 10-change-cgroup.conf\n        contents: |\n                    \n
  \ - name: docker.service\n    enabled: true\n    contents: |\n    dropins:\n      -
  name: 10-giantswarm-extra-args.conf\n        contents: |\n                [Service]\n
  \     CPUAccounting=true\n      MemoryAccounting=true\n      Slice=kubereserved.slice\n
  \     Environment=&#34;DOCKER_CGROUPS=--exec-opt native.cgroupdriver=cgroupfs --cgroup-parent=/kubereserved.slice
  --log-opt max-size=25m --log-opt max-file=2 --log-opt labels=io.kubernetes.container.hash,io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace,io.kubernetes.pod.uid&#34;\n
  \     Environment=&#34;DOCKER_OPT_BIP=--bip=DockerDaemonCIDR&#34;\n      Environment=&#34;DOCKER_OPTS=--live-restore
  --icc=false --userland-proxy=false&#34;\n      \n  - name: k8s-setup-network-env.service\n
  \   enabled: true\n    contents: |\n            [Unit]\n      Description=k8s-setup-network-env
  Service\n      Wants=network.target docker.service wait-for-domains.service\n      After=network.target
  docker.service wait-for-domains.service\n      [Service]\n      Type=oneshot\n      TimeoutStartSec=0\n
  \     Environment=&#34;IMAGE=DockerNetworkSetupImage&#34;\n      Environment=&#34;NAME=%p.service&#34;\n
  \     ExecStartPre=/usr/bin/mkdir -p /opt/bin/\n      ExecStartPre=/usr/bin/docker
  pull $IMAGE\n      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME\n      ExecStartPre=-/usr/bin/docker
  rm -f $NAME\n      ExecStart=/usr/bin/docker run --rm --net=host -v /etc:/etc --name
  $NAME $IMAGE\n      ExecStop=-/usr/bin/docker stop -t 10 $NAME\n      ExecStopPost=-/usr/bin/docker
  rm -f $NAME\n      [Install]\n      WantedBy=multi-user.target\n      \n  - name:
  etcd3.service\n    enabled: true\n    contents: |\n            [Unit]\n      Description=etcd3\n
  \     Wants=k8s-setup-network-env.service\n      After=k8s-setup-network-env.service\n
  \     Conflicts=etcd.service etcd2.service\n      StartLimitIntervalSec=0\n      [Service]\n
  \     Restart=always\n      RestartSec=0\n      TimeoutStopSec=10\n      LimitNOFILE=40000\n
  \     CPUAccounting=true\n      MemoryAccounting=true\n      Slice=kubereserved.slice\n
  \     Environment=IMAGE=\n  - name: etcd3-defragmentation.service\n    enabled:
  false\n    contents: |\n            [Unit]\n      Description=etcd defragmentation
  job\n      After=docker.service etcd3.service\n      Requires=docker.service etcd3.service\n
  \     [Service]\n      Type=oneshot\n      EnvironmentFile=/etc/network-environment\n
  \     Environment=IMAGE=RegistryDomain/EtcdImage\n      Environment=NAME=%p.service\n
  \     ExecStartPre=-/usr/bin/docker stop  $NAME\n      ExecStartPre=-/usr/bin/docker
  rm  $NAME\n      ExecStartPre=-/usr/bin/docker pull $IMAGE\n      ExecStart=/usr/bin/docker
  run \\\n        -v /etc/kubernetes/ssl/etcd/:/etc/etcd \\\n        --net=host  \\\n
  \       -e ETCDCTL_API=3 \\\n        --name $NAME \\\n        $IMAGE \\\n        etcdctl
  \\\n        --endpoints https://127.0.0.1:2379 \\\n        --cacert /etc/etcd/server-ca.pem
  \\\n        --cert /etc/etcd/server-crt.pem \\\n        --key /etc/etcd/server-key.pem
  \\\n        defrag \\\n        --command-timeout=60s \\\n        --dial-timeout=60s
  \\\n        --keepalive-timeout=25s\n      [Install]\n      WantedBy=multi-user.target\n
  \     \n  - name: etcd3-defragmentation.timer\n    enabled: true\n    contents:
  |\n            [Unit]\n      Description=Execute etcd3-defragmentation every day
  at 3.30AM UTC\n      [Timer]\n      OnCalendar=*-*-* 03:30:00 UTC\n      [Install]\n
  \     WantedBy=multi-user.target\n      \n  - name: k8s-setup-download-hyperkube.service\n
  \   enabled: true\n    contents: |\n            [Unit]\n      Description=Pulls
  hyperkube binary from image to local FS\n      After=docker.service\n      Requires=docker.service\n
  \     [Service]\n      Type=oneshot\n      RemainAfterExit=yes\n      TimeoutStartSec=0\n
  \     Environment=&#34;IMAGE=RegistryDomain/K8sImage&#34;\n      Environment=&#34;NAME=%p.service&#34;\n
  \     ExecStartPre=/bin/bash -c &#34;/usr/bin/docker create --name $NAME $IMAGE&#34;\n
  \     ExecStart=/bin/bash -c &#34;/usr/bin/docker cp $NAME:/hyperkube /opt/bin/hyperkube&#34;\n
  \     ExecStartPost=/bin/bash -c &#34;/usr/bin/docker rm $NAME&#34;\n      [Install]\n
  \     WantedBy=multi-user.target\n      \n  - name: k8s-kubelet.service\n    enabled:
  true\n    contents: |\n            [Unit]\n      Wants=k8s-setup-network-env.service
  k8s-setup-kubelet-config.service k8s-setup-download-hyperkube.service\n      After=k8s-setup-network-env.service
  k8s-setup-kubelet-config.service k8s-setup-download-hyperkube.service\n      Description=k8s-kubelet\n
  \     StartLimitIntervalSec=0\n      [Service]\n      TimeoutStartSec=300\n      Restart=always\n
  \     RestartSec=0\n      TimeoutStopSec=10\n      Slice=kubereserved.slice\n      CPUAccounting=true\n
  \     MemoryAccounting=true\n      Environment=&#34;ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/etcd/server-ca.pem&#34;\n
  \     Environment=&#34;ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd/server-crt.pem&#34;\n
  \     Environment=&#34;ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd/server-key.pem&#34;\n
  \     EnvironmentFile=/etc/network-environment\n      ExecStart=/opt/bin/hyperkube
  kubelet \\\n        kubeletArg1 \\\n        kubeletArg2 \\\n        --node-ip=${DEFAULT_IPV4}
  \\\n        --config=/etc/kubernetes/config/kubelet.yaml \\\n        --enable-server
  \\\n        --logtostderr=true \\\n        --cloud-provider=aws \\\n        --image-pull-progress-deadline=SomeProgressDeadline
  \\\n        --network-plugin=cni \\\n        --register-node=true \\\n        --register-with-taints=node-role.kubernetes.io/master=:NoSchedule
  \\\n        --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.yaml \\\n        --node-labels=&#34;node.kubernetes.io/master,node-role.kubernetes.io/master,kubernetes.io/role=master,role=master,ip=${DEFAULT_IPV4},\n
  \ - name: etcd2.service\n    enabled: false\n    mask: true\n  - name: update-engine.service\n
  \   enabled: false\n    mask: true\n  - name: locksmithd.service\n    enabled: false\n
  \   mask: true\n  - name: fleet.service\n    enabled: false\n    mask: true\n  -
  name: fleet.socket\n    enabled: false\n    mask: true\n  - name: flanneld.service\n
  \   enabled: false\n    mask: true\n  - name: systemd-networkd-wait-online.service\n
  \   enabled: false\n    mask: true\n  - name: k8s-addons.service\n    enabled: true\n
  \   contents: |\n            [Unit]\n      Description=Kubernetes Addons\n      Wants=k8s-kubelet.service
  k8s-setup-network-env.service\n      After=k8s-kubelet.service k8s-setup-network-env.service\n
  \     [Service]\n      Type=oneshot\n      ExecStart=/opt/k8s-addons\n      # https://github.com/kubernetes/kubernetes/issues/71078\n
  \     ExecStartPost=/usr/bin/systemctl restart k8s-kubelet.service\n      [Install]\n
  \     WantedBy=multi-user.target\n      \n  - name: debug-tools.service\n    enabled:
  true\n    contents: |\n            [Unit]\n      Description=Install calicoctl and
  crictl tools\n      After=network.target\n      [Service]\n      Type=oneshot\n
  \     ExecStart=/opt/install-debug-tools\n      [Install]\n      WantedBy=multi-user.target\n
  \     \n\nstorage:\n  files:\n    - path: /etc/ssh/trusted-user-ca-keys.pem\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;base64,SSOPublicKey\n\"\n
  \   - path: /srv/calico-all.yaml\n      filesystem: root\n      mode: 0644\n      contents:\n
  \       source: \"data:text/plain;charset=utf-8;base64,# CALICO HAS SEPARATE MANIFEST
  FOR AZURE\n# the azure manifest can be found in: https://github.com/giantswarm/azure-operator/blob/master/service/controller/vX/cloudconfig/template.go\n#
  where X is the version of azure operator\n#\n# Extra changes:\n#  - Added resource
  limits to calico-node and calico-kube-controller.\n#  - Added resource limits to
  install-cni.\n#  - Added &#39;priorityClassName: system-cluster-critical&#39; to
  calico daemonset.\n#\n# Calico Version v3.9.1\n# https://docs.projectcalico.org/v3.9/release-notes/\n#
  This manifest includes the following component versions:\n#   calico/node:v3.9.1\n#
  \  calico/cni:v3.9.1\n#   calico/kube-controllers:v3.9.1\n\n# This ConfigMap is
  used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion:
  v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Configure
  this with the location of your etcd cluster.\n  etcd_endpoints: &#34;https://EtcdDomain:1234&#34;\n\n
  \ # If you&#39;re using TLS enabled etcd uncomment the following.\n  # You must
  also populate the Secret below with these files.\n  etcd_ca: &#34;/calico-secrets/client-ca.pem&#34;\n
  \ etcd_cert: &#34;/calico-secrets/client-crt.pem&#34;\n  etcd_key: &#34;/calico-secrets/client-key.pem&#34;\n
  \ # Typha is disabled.\n  typha_service_name: &#34;none&#34;\n  # Configure the
  backend to use.\n  calico_backend: &#34;bird&#34;\n\n  # Configure the MTU to use\n
  \ veth_mtu: &#34;CalicoMTU&#34;\n\n  # The CNI network configuration to install
  on each node.  The special\n  # values in this config will be automatically populated.\n
  \ cni_network_config: |-\n    {\n      &#34;name&#34;: &#34;k8s-pod-network&#34;,\n
  \     &#34;cniVersion&#34;: &#34;0.3.1&#34;,\n      &#34;plugins&#34;: [\n        {\n
  \         &#34;type&#34;: &#34;calico&#34;,\n          &#34;log_level&#34;: &#34;info&#34;,\n
  \         &#34;nodename&#34;: &#34;__KUBERNETES_NODE_NAME__&#34;,\n          &#34;etcd_endpoints&#34;:
  &#34;__ETCD_ENDPOINTS__&#34;,\n          &#34;etcd_key_file&#34;: &#34;__ETCD_KEY_FILE__&#34;,\n
  \         &#34;etcd_cert_file&#34;: &#34;__ETCD_CERT_FILE__&#34;,\n          &#34;etcd_ca_cert_file&#34;:
  &#34;__ETCD_CA_CERT_FILE__&#34;,\n          &#34;mtu&#34;: __CNI_MTU__,\n          &#34;ipam&#34;:
  {\n              &#34;type&#34;: &#34;calico-ipam&#34;\n          },\n          &#34;policy&#34;:
  {\n              &#34;type&#34;: &#34;k8s&#34;\n          },\n          &#34;kubernetes&#34;:
  {\n              &#34;kubeconfig&#34;: &#34;__KUBECONFIG_FILEPATH__&#34;\n          }\n
  \       },\n        {\n          &#34;type&#34;: &#34;portmap&#34;,\n          &#34;snat&#34;:
  true,\n          &#34;capabilities&#34;: {&#34;portMappings&#34;: true}\n        }\n
  \     ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n
  \ scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion:
  apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name:
  ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n
  \ version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular:
  ipamblock\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n
  \ name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group:
  crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural:
  blockaffinities\n    singular: blockaffinity\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n
  \ scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n\n---\n\napiVersion:
  apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name:
  ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n
  \ version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular:
  ipamconfig\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n
  \ name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n
  \ version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular:
  bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n
  \ name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group:
  crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural:
  bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n
  \ scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n
  \ scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion:
  apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name:
  clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n
  \ version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n
  \   singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n
  \ scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion:
  apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name:
  globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n
  \ version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n
  \   singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind:
  CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n
  \ scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind:
  NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n#
  This manifest installs the calico-node container, as well\n# as the CNI plugins
  and network config on\n# each master and worker node in a Kubernetes cluster.\nkind:
  DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n
  \ labels:\n    giantswarm.io/service-type: managed\n    k8s-app: calico-node\nspec:\n
  \ selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n    type:
  RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n
  \     labels:\n        k8s-app: calico-node\n        giantswarm.io/service-type:
  managed\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration
  below,\n        # marks the pod as a critical add-on, ensuring it gets\n        #
  priority scheduling and that its resources are reserved\n        # if it ever gets
  evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;\n    spec:\n
  \     nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n
  \     tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n
  \       - effect: NoSchedule\n          operator: Exists\n        # Mark the pod
  as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator:
  Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName:
  calico-node\n      # Minimize downtime during a rolling upgrade or deletion; tell
  Kubernetes to do a &#34;force\n      # deletion&#34;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n
  \     terminationGracePeriodSeconds: 0\n      priorityClassName: system-cluster-critical\n
  \     initContainers:\n        # This container installs the CNI binaries\n        #
  and CNI network config file on each node.\n        - name: install-cni\n          image:
  RegistryDomain/giantswarm/cni:v3.9.1\n          command: [&#34;/install-cni.sh&#34;]\n
  \         env:\n            # Name of the CNI config file to create.\n            -
  name: CNI_CONF_NAME\n              value: &#34;10-calico.conflist&#34;\n            #
  The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n
  \             valueFrom:\n                configMapKeyRef:\n                  name:
  calico-config\n                  key: cni_network_config\n            # The location
  of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n
  \               configMapKeyRef:\n                  name: calico-config\n                  key:
  etcd_endpoints\n            # CNI MTU Config variable\n            - name: CNI_MTU\n
  \             valueFrom:\n                configMapKeyRef:\n                  name:
  calico-config\n                  key: veth_mtu\n            # Prevents the container
  from sleeping forever.\n            - name: SLEEP\n              value: &#34;false&#34;\n
  \         resources:\n            requests:\n              cpu: 50m\n              memory:
  100Mi\n            limits:\n              cpu: 50m\n              memory: 100Mi\n
  \         volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name:
  cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n
  \           - mountPath: /calico-secrets\n              name: etcd-certs\n      containers:\n
  \       # Runs calico-node container on each Kubernetes node.  This\n        # container
  programs network policy and routes on each\n        # host.\n        - name: calico-node\n
  \         image: RegistryDomain/giantswarm/node:v3.9.1\n          env:\n            #
  The location of the Calico etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n
  \               configMapKeyRef:\n                  name: calico-config\n                  key:
  etcd_endpoints\n            # Location of the CA certificate for etcd.\n            -
  name: ETCD_CA_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n
  \                 name: calico-config\n                  key: etcd_ca\n            #
  Location of the client key for etcd.\n            - name: ETCD_KEY_FILE\n              valueFrom:\n
  \               configMapKeyRef:\n                  name: calico-config\n                  key:
  etcd_key\n            # Location of the client certificate for etcd.\n            -
  name: ETCD_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n
  \                 name: calico-config\n                  key: etcd_cert\n            #
  Set noderef for node controller.\n            - name: CALICO_K8S_NODE_REF\n              valueFrom:\n
  \               fieldRef:\n                  fieldPath: spec.nodeName\n            #
  Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n
  \               configMapKeyRef:\n                  name: calico-config\n                  key:
  calico_backend\n            # Cluster type to identify the deployment type\n            -
  name: CLUSTER_TYPE\n              value: &#34;k8s,bgp&#34;\n            # Auto-detect
  the BGP IP address.\n            - name: IP\n              value: &#34;autodetect&#34;\n
  \           # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value:
  &#34;Always&#34;\n            # Set MTU for tunnel device used if ipip is enabled\n
  \           - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n
  \                 name: calico-config\n                  key: veth_mtu\n            #
  The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            #
  chosen from this range. Changing this value after installation will have\n            #
  no effect. This should fall within `--cluster-cidr`.\n            - name: CALICO_IPV4POOL_CIDR\n
  \             value: &#34;CalicoSubnet/CalicoCIDR&#34;\n            # Disable file
  logging so `kubectl logs` works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n
  \             value: &#34;true&#34;\n            # Set Felix endpoint to host default
  action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value:
  &#34;ACCEPT&#34;\n            # Disable IPv6 on Kubernetes.\n            - name:
  FELIX_IPV6SUPPORT\n              value: &#34;false&#34;\n            # Set Felix
  logging to &#34;info&#34;\n            - name: FELIX_LOGSEVERITYSCREEN\n              value:
  &#34;Warning&#34;\n            - name: FELIX_HEALTHENABLED\n              value:
  &#34;true&#34;\n            - name: FELIX_PROMETHEUSMETRICSENABLED\n              value:
  &#34;true&#34;\n          securityContext:\n            privileged: true\n          resources:\n
  \           requests:\n              cpu: 250m\n              memory: 150Mi\n          livenessProbe:\n
  \           exec:\n              command:\n              - /bin/calico-node\n              -
  -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n
  \           failureThreshold: 6\n          readinessProbe:\n            exec:\n
  \             command:\n                - /bin/calico-node\n                - -felix-ready\n
  \               - -bird-ready\n            periodSeconds: 10\n          volumeMounts:\n
  \           - mountPath: /lib/modules\n              name: lib-modules\n              readOnly:
  true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n
  \             readOnly: false\n            - mountPath: /var/run/calico\n              name:
  var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n
  \             name: var-lib-calico\n              readOnly: false\n            -
  mountPath: /calico-secrets\n              name: etcd-certs\n      volumes:\n        #
  Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path:
  /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path:
  /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path:
  /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path:
  /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n
  \       - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n
  \       - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n
  \       # Mount in the etcd TLS secrets.\n        # See https://kubernetes.io/docs/concepts/configuration/secret/\n
  \       - name: etcd-certs\n          hostPath:\n            path: /etc/kubernetes/ssl/etcd\n---\n\napiVersion:
  v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n\n---\n#
  This manifest deploys the Calico Kubernetes controllers.\n# See https://github.com/projectcalico/kube-controllers\napiVersion:
  apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace:
  kube-system\n  labels:\n    giantswarm.io/service-type: managed\n    k8s-app: calico-kube-controllers\n
  \ annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;\nspec:\n
  \ # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n
  \   matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type:
  Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace:
  kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n        giantswarm.io/service-type:
  managed\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n        kubernetes.io/role:
  master\n      # The controllers must run in the host network namespace so that\n
  \     # it isn&#39;t governed by policy that would prevent it from working.\n      hostNetwork:
  true\n      tolerations:\n        # Mark the pod as a critical add-on for rescheduling.\n
  \       - key: CriticalAddonsOnly\n          operator: Exists\n        - key: node-role.kubernetes.io/master\n
  \         effect: NoSchedule\n      serviceAccountName: calico-kube-controllers\n
  \     priorityClassName: system-cluster-critical\n      containers:\n        - name:
  calico-kube-controllers\n          image: RegistryDomain/giantswarm/kube-controllers:v3.9.1\n
  \         env:\n            # The location of the Calico etcd cluster.\n            -
  name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n
  \                 name: calico-config\n                  key: etcd_endpoints\n            #
  Location of the CA certificate for etcd.\n            - name: ETCD_CA_CERT_FILE\n
  \             valueFrom:\n                configMapKeyRef:\n                  name:
  calico-config\n                  key: etcd_ca\n            # Location of the client
  key for etcd.\n            - name: ETCD_KEY_FILE\n              valueFrom:\n                configMapKeyRef:\n
  \                 name: calico-config\n                  key: etcd_key\n            #
  Location of the client certificate for etcd.\n            - name: ETCD_CERT_FILE\n
  \             valueFrom:\n                configMapKeyRef:\n                  name:
  calico-config\n                  key: etcd_cert\n            # Choose which controllers
  to run.\n            - name: ENABLED_CONTROLLERS\n              value: policy,profile,workloadendpoint,node,serviceaccount\n
  \         volumeMounts:\n            # Mount in the etcd TLS secrets.\n            -
  mountPath: /calico-secrets\n              name: etcd-certs\n          resources:\n
  \           requests:\n              cpu: 250m\n              memory: 100Mi\n            limits:\n
  \             cpu: 250m\n              memory: 100Mi\n          readinessProbe:\n
  \           exec:\n              command:\n                - /usr/bin/check-status\n
  \               - -r\n      volumes:\n        # Mount in the etcd TLS secrets with
  mode 400.\n        # See https://kubernetes.io/docs/concepts/configuration/secret/\n
  \       - name: etcd-certs\n          hostPath:\n            path: /etc/kubernetes/ssl/etcd\n\n---\n\napiVersion:
  v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace:
  kube-system\n---\n---\n# Include a clusterrole for the kube-controllers component,\n#
  and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion:
  rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n-
  apiGroups:\n  - &#34;&#34;\n  resources:\n  - nodes\n  - pods\n  - namespaces\n
  \ - serviceaccounts\n  verbs:\n  - watch\n  - list\n  - get\n- apiGroups:\n  - networking.k8s.io\n
  \ resources:\n  - networkpolicies\n  verbs:\n  - watch\n  - list\n  # IPAM resources
  are manipulated when nodes are deleted.\n- apiGroups:\n  - crd.projectcalico.org\n
  \ resources:\n  - ippools\n  verbs:\n  - list\n- apiGroups:\n  - crd.projectcalico.org\n
  \ resources:\n  - blockaffinities\n  - ipamblocks\n  - ipamhandles\n  verbs:\n  -
  get\n  - list\n  - create\n  - update\n  - delete\n- apiGroups:\n  - crd.projectcalico.org\n
  \ resources:\n  - clusterinformations\n  verbs:\n  - get\n  - create\n  - update\n---\nkind:
  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name:
  calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind:
  ClusterRole\n  name: calico-kube-controllers\nsubjects:\n  - kind: ServiceAccount\n
  \   name: calico-kube-controllers\n    namespace: kube-system\n---\n# Include a
  clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind:
  ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n
  \ # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [&#34;&#34;]\n
  \   resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      -
  get\n  - apiGroups: [&#34;&#34;]\n    resources:\n      - endpoints\n      - services\n
  \   verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n
  \     - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [&#34;&#34;]\n
  \   resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable
  flag.\n      - patch\n      # Calico stores some configuration information in node
  annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n
  \ - apiGroups: [&#34;networking.k8s.io&#34;]\n    resources:\n      - networkpolicies\n
  \   verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n
  \ - apiGroups: [&#34;&#34;]\n    resources:\n      - pods\n      - namespaces\n
  \     - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin
  patches pods/status.\n  - apiGroups: [&#34;&#34;]\n    resources:\n      - pods/status\n
  \   verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups:
  [&#34;crd.projectcalico.org&#34;]\n    resources:\n      - globalfelixconfigs\n
  \     - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      -
  bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n
  \     - globalnetworksets\n      - networkpolicies\n      - networksets\n      -
  clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n
  \     - get\n      - list\n      - watch\n  # Calico must create and update some
  CRDs on startup.\n  - apiGroups: [&#34;crd.projectcalico.org&#34;]\n    resources:\n
  \     - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n
  \     - create\n      - update\n  # Calico stores some configuration information
  on the node.\n  - apiGroups: [&#34;&#34;]\n    resources:\n      - nodes\n    verbs:\n
  \     - get\n      - list\n      - watch\n  # These permissions are only requried
  for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n
  \ - apiGroups: [&#34;crd.projectcalico.org&#34;]\n    resources:\n      - bgpconfigurations\n
  \     - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions
  are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [&#34;crd.projectcalico.org&#34;]\n
  \   resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n
  \   verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n
  \ - apiGroups: [&#34;crd.projectcalico.org&#34;]\n    resources:\n      - ipamconfigs\n
  \   verbs:\n      - get\n  # Block affinities must also be watchable by confd for
  route aggregation.\n  - apiGroups: [&#34;crd.projectcalico.org&#34;]\n    resources:\n
  \     - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration
  needs to get daemonsets. These permissions can be\n  # removed if not upgrading
  from an installation using host-local IPAM.\n  - apiGroups: [&#34;apps&#34;]\n    resources:\n
  \     - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind:
  ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n
  \ kind: ClusterRole\n  name: calico-node\nsubjects:\n  - kind: ServiceAccount\n
  \   name: calico-node\n    namespace: kube-system\n\"\n    - path: /srv/ingress-controller-svc.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/port: &#34;10254&#34;\n
  \   prometheus.io/scrape: &#34;true&#34;\n  name: nginx-ingress-controller\n  namespace:
  kube-system\n  labels:\n    k8s-app: nginx-ingress-controller\nspec:\n  type: NodePort\n
  \ ports:\n  - name: http\n    port: 80\n    nodePort: 30010\n    protocol: TCP\n
  \   targetPort: 80\n  - name: https\n    port: 443\n    nodePort: 30011\n    protocol:
  TCP\n    targetPort: 443\n  selector:\n    k8s-app: nginx-ingress-controller\n\"\n\n
  \   - path: /etc/kubernetes/config/proxy-config.yml\n      filesystem: root\n      mode:
  0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  kubeproxy.config.k8s.io/v1alpha1\nclientConnection:\n  kubeconfig: /etc/kubernetes/config/proxy-kubeconfig.yaml\nkind:
  KubeProxyConfiguration\nmode: iptables\nresourceContainer: /kube-proxy\nclusterCIDR:
  CalicoSubnet/CalicoCIDR\nmetricsBindAddress: 0.0.0.0:10249\n\"\n\n    - path: /srv/kube-proxy-config.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  kubeproxy.config.k8s.io/v1alpha1\nclientConnection:\n  kubeconfig: /etc/kubernetes/config/proxy-kubeconfig.yaml\nkind:
  KubeProxyConfiguration\nmode: iptables\nresourceContainer: /kube-proxy\nclusterCIDR:
  CalicoSubnet/CalicoCIDR\nmetricsBindAddress: 0.0.0.0:10249\n\"\n\n    - path: /srv/kube-proxy-sa.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: ServiceAccount\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\"\n\n
  \   - path: /srv/kube-proxy-ds.yaml\n      filesystem: root\n      mode: 0644\n
  \     contents:\n        source: \"data:text/plain;charset=utf-8;base64,kind: DaemonSet\napiVersion:
  extensions/v1beta1\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n
  \   component: kube-proxy\n    k8s-app: kube-proxy\n    kubernetes.io/cluster-service:
  &#34;true&#34;\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-proxy\n
  \ updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable:
  1\n  template:\n    metadata:\n      labels:\n        component: kube-proxy\n        k8s-app:
  kube-proxy\n        kubernetes.io/cluster-service: &#34;true&#34;\n      annotations:\n
  \       scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;\n    spec:\n      tolerations:\n
  \     # Mark the pod as a critical add-on for rescheduling.\n      - key: CriticalAddonsOnly\n
  \       operator: Exists\n      # Make sure the pod gets scheduled on all nodes.\n
  \     - operator: Exists\n      hostNetwork: true\n      priorityClassName: system-node-critical\n
  \     serviceAccountName: kube-proxy\n      containers:\n        - name: kube-proxy\n
  \         image: RegistryDomain/K8sImage\n          command:\n          - /hyperkube\n
  \         - kube-proxy\n          - --config=/etc/kubernetes/config/proxy-config.yml\n
  \         - --v=2\n          livenessProbe:\n            httpGet:\n              path:
  /healthz\n              port: 10256\n            initialDelaySeconds: 10\n            periodSeconds:
  3\n          resources:\n            requests:\n              memory: &#34;80Mi&#34;\n
  \             cpu: &#34;75m&#34;\n          securityContext:\n            privileged:
  true\n          volumeMounts:\n          - mountPath: /etc/ssl/certs\n            name:
  ssl-certs-host\n            readOnly: true\n          - mountPath: /etc/kubernetes/config/\n
  \           name: k8s-config\n          - mountPath: /etc/kubernetes/kubeconfig/\n
  \           name: k8s-kubeconfig\n            readOnly: true\n          - mountPath:
  /etc/kubernetes/ssl\n            name: ssl-certs-kubernetes\n            readOnly:
  true\n          - mountPath: /lib/modules\n            name: lib-modules\n            readOnly:
  true\n      volumes:\n      - hostPath:\n          path: /etc/kubernetes/config/\n
  \       name: k8s-config\n      - hostPath:\n          path: /etc/kubernetes/config/\n
  \       name: k8s-kubeconfig\n      - hostPath:\n          path: /etc/kubernetes/ssl\n
  \       name: ssl-certs-kubernetes\n      - hostPath:\n          path: /usr/share/ca-certificates\n
  \       name: ssl-certs-host\n      - hostPath:\n          path: /lib/modules\n
  \       name: lib-modules\n\"\n\n    - path: /srv/rbac_bindings.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,##
  User\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n
  \ name: giantswarm-admin\nsubjects:\n- kind: User\n  name: APIDomain\n  apiGroup:
  rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n
  \ apiGroup: rbac.authorization.k8s.io\n---\n## Worker\nkind: ClusterRoleBinding\napiVersion:
  rbac.authorization.k8s.io/v1\nmetadata:\n  name: kubelet\nsubjects:\n- kind: User\n
  \ name: K8sKubeletDomain\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind:
  ClusterRole\n  name: system:node\n  apiGroup: rbac.authorization.k8s.io\n---\nkind:
  ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name:
  proxy\nsubjects:\n- kind: User\n  name: K8sKubeletDomain\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n
  \ kind: ClusterRole\n  name: system:node-proxier\n  apiGroup: rbac.authorization.k8s.io\n---\n##
  Master\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n
  \ name: kube-controller-manager\nsubjects:\n- kind: User\n  name: APIDomain\n  apiGroup:
  rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: system:kube-controller-manager\n
  \ apiGroup: rbac.authorization.k8s.io\n---\nkind: ClusterRoleBinding\napiVersion:
  rbac.authorization.k8s.io/v1\nmetadata:\n  name: kube-scheduler\nsubjects:\n- kind:
  User\n  name: APIDomain\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind:
  ClusterRole\n  name: system:kube-scheduler\n  apiGroup: rbac.authorization.k8s.io\n---\n##
  node-operator\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n
  \ name: node-operator\nsubjects:\n- kind: User\n  name: node-operator.ClusterBaseDomain\n
  \ apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: node-operator\n
  \ apiGroup: rbac.authorization.k8s.io\n---\n## prometheus-external is prometheus
  from host cluster\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n
  \ name: prometheus-external\nsubjects:\n- kind: User\n  name: prometheus.ClusterBaseDomain\n
  \ apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: prometheus-external\n
  \ apiGroup: rbac.authorization.k8s.io\n\"\n\n    - path: /srv/rbac_roles.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,##
  node-operator\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n
  \ name: node-operator\nrules:\n- apiGroups: [&#34;&#34;]\n  resources: [&#34;nodes&#34;]\n
  \ verbs: [&#34;patch&#34;]\n- apiGroups: [&#34;&#34;]\n  resources: [&#34;pods&#34;]\n
  \ verbs: [&#34;list&#34;, &#34;delete&#34;]\n---\n## prometheus-external\nkind:
  ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: prometheus-external\nrules:\n-
  apiGroups: [&#34;&#34;]\n  resources:\n  - nodes\n  - nodes/proxy\n  - services\n
  \ - endpoints\n  - pods\n  verbs: [&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;]\n-
  apiGroups:\n  - extensions\n  resources:\n  - ingresses\n  verbs: [&#34;get&#34;,
  &#34;list&#34;, &#34;watch&#34;]\n- nonResourceURLs: [&#34;/metrics&#34;]\n  verbs:
  [&#34;get&#34;]\n\"\n\n    - path: /srv/priority_classes.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: giantswarm-critical\nvalue:
  1000000000\nglobalDefault: false\ndescription: &#34;This priority class is used
  by giantswarm kubernetes components.&#34;\n\"\n\n    - path: /srv/psp_policies.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: privileged\n  annotations:\n
  \   seccomp.security.alpha.kubernetes.io/allowedProfileNames: &#39;*&#39;\nspec:\n
  \ allowPrivilegeEscalation: true\n  allowedCapabilities:\n  - &#39;*&#39;\n  fsGroup:\n
  \   rule: RunAsAny\n  privileged: true\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n
  \   rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n  - &#39;*&#39;\n
  \ hostPID: true\n  hostIPC: true\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n
  \   max: 65536\n---\n---\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n
  \ name: restricted\nspec:\n  privileged: false\n  allowPrivilegeEscalation: false\n
  \ runAsUser:\n    ranges:\n      - max: 65535\n        min: 1000\n    rule: MustRunAs\n
  \ seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: &#39;MustRunAs&#39;\n
  \   ranges:\n      - min: 1\n        max: 65535\n  fsGroup:\n    rule: &#39;MustRunAs&#39;\n
  \   ranges:\n      - min: 1\n        max: 65535\n  volumes:\n  - &#39;secret&#39;\n
  \ - &#39;configMap&#39;\n  hostPID: false\n  hostIPC: false\n  hostNetwork: false\n
  \ readOnlyRootFilesystem: false\n\"\n\n    - path: /srv/psp_roles.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,#
  restrictedPSP grants access to use\n# the restricted PSP.\napiVersion: rbac.authorization.k8s.io/v1\nkind:
  ClusterRole\nmetadata:\n  name: restricted-psp-user\nrules:\n- apiGroups:\n  - extensions\n
  \ resources:\n  - podsecuritypolicies\n  resourceNames:\n  - restricted\n  verbs:\n
  \ - use\n---\n# privilegedPSP grants access to use the privileged\n# PSP.\napiVersion:
  rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: privileged-psp-user\nrules:\n-
  apiGroups:\n  - extensions\n  resources:\n  - podsecuritypolicies\n  resourceNames:\n
  \ - privileged\n  verbs:\n  - use\n\"\n\n    - path: /srv/psp_binding.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: privileged-psp-users\nsubjects:\n-
  kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- kind: ServiceAccount\n
  \ name: calico-kube-controllers\n  namespace: kube-system\n- kind: ServiceAccount\n
  \ name: kube-proxy\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n
  \ kind: ClusterRole\n  name: privileged-psp-user\n---\n# grants the restricted PSP
  role to\n# the all authenticated users.\napiVersion: rbac.authorization.k8s.io/v1\nkind:
  ClusterRoleBinding\nmetadata:\n    name: restricted-psp-users\nsubjects:\n- kind:
  Group\n  apiGroup: rbac.authorization.k8s.io\n  name: system:authenticated\nroleRef:\n
  \ apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: restricted-psp-user\n\"\n\n
  \   - path: /srv/network_policies.yaml\n      filesystem: root\n      mode: 0644\n
  \     contents:\n        source: \"data:text/plain;charset=utf-8;base64,---\napiVersion:
  networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n
  \ podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\"\n\n    - path: /opt/wait-for-domains\n
  \     filesystem: root\n      mode: 0544\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,#!/bin/bash\ndomains=&#34;EtcdDomain
  APIDomain quay.io&#34;\n\nfor domain in $domains; do\nuntil nslookup $domain; do\n
  \   echo &#34;Waiting for domain $domain to be available&#34;\n    sleep 5\ndone\n\necho
  &#34;Successfully resolved domain $domain&#34;\ndone\n\"\n\n    - path: /opt/k8s-addons\n
  \     filesystem: root\n      mode: 0544\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,#!/bin/bash\n\nexport
  KUBECONFIG=/etc/kubernetes/kubeconfig/addons.yaml\n# kubectl 1.12.2\nKUBECTL=RegistryDomain/giantswarm/docker-kubectl:f5cae44c480bd797dc770dd5f62d40b74063c0d7\n\n/usr/bin/docker
  pull $KUBECTL\n\n# wait for healthy master\nwhile [ &#34;$(/usr/bin/docker run -e
  KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL
  get cs | grep Healthy | wc -l)&#34; -ne &#34;3&#34; ]; do sleep 1 &amp;&amp; echo
  &#39;Waiting for healthy k8s&#39;; done\n\n# label namespaces (required for network
  egress policies)\nNAMESPACES=&#34;default kube-system&#34;\nfor namespace in ${NAMESPACES}\ndo\n
  \   if ! /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL get namespaces -l name=${namespace} | grep ${namespace}; then\n        while\n
  \           /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL label namespace ${namespace} name=${namespace}\n            [ &#34;$?&#34;
  -ne &#34;0&#34; ]\n        do\n            echo &#34;failed to label namespace ${namespace},
  retrying in 5 sec&#34;\n            sleep 5s\n        done\n    fi\ndone\n\n# apply
  Security bootstrap (RBAC and PSP)\nSECURITY_FILES=&#34;&#34;\nSECURITY_FILES=&#34;${SECURITY_FILES}
  rbac_bindings.yaml&#34;\nSECURITY_FILES=&#34;${SECURITY_FILES} rbac_roles.yaml&#34;\nSECURITY_FILES=&#34;${SECURITY_FILES}
  psp_policies.yaml&#34;\nSECURITY_FILES=&#34;${SECURITY_FILES} psp_roles.yaml&#34;\nSECURITY_FILES=&#34;${SECURITY_FILES}
  psp_binding.yaml&#34;\n\nfor manifest in $SECURITY_FILES\ndo\n    while\n        /usr/bin/docker
  run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL apply -f /srv/$manifest\n        [ &#34;$?&#34; -ne &#34;0&#34; ]\n    do\n
  \       echo &#34;failed to apply /srv/$manifest, retrying in 5 sec&#34;\n        sleep
  5s\n    done\ndone\n\n# check for other master and remove it\nTHIS_MACHINE=$(cat
  /etc/hostname)\nfor master in $(/usr/bin/docker run -e KUBECONFIG=${KUBECONFIG}
  --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL get nodes --no-headers=true
  --selector role=master | awk &#39;{print $1}&#39;)\ndo\n    if [ &#34;$master&#34;
  != &#34;$THIS_MACHINE&#34; ]; then\n        /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG}
  --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL delete node $master\n
  \   fi\ndone\n\n# wait for etcd dns (return code 35 is bad certificate which is
  good enough here)\nwhile\n    curl &#34;https://EtcdDomain:1234&#34; -k 2&gt;/dev/null
  &gt;/dev/null\n    RET_CODE=$?\n    [ &#34;$RET_CODE&#34; -ne &#34;35&#34; ]\ndo\n
  \   echo &#34;Waiting for etcd to be ready . . &#34;\n    sleep 3s\ndone\n\n# create
  kube-proxy configmap\nwhile\n    /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG}
  --net=host --rm -v /srv:/srv $KUBECTL create configmap kube-proxy --from-file=kube-proxy.yaml=/srv/kube-proxy-config.yaml
  -o yaml --dry-run \\\n    | /usr/bin/docker run  -i --log-driver=none -a stdin -a
  stdout -a stderr -e KUBECONFIG=${KUBECONFIG} -v /etc/kubernetes:/etc/kubernetes
  --net=host --rm $KUBECTL apply -n kube-system -f -\n    [ &#34;$?&#34; -ne &#34;0&#34;
  ]\ndo\n    echo &#34;failed to configure kube-proxy from /srv/kube-proxy-config.yaml,
  retrying in 5 sec&#34;\n    sleep 5s\ndone\n\n# install kube-proxy\nPROXY_MANIFESTS=&#34;kube-proxy-sa.yaml
  kube-proxy-ds.yaml&#34;\nfor manifest in $PROXY_MANIFESTS\ndo\n    while\n        /usr/bin/docker
  run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL apply -f /srv/$manifest\n        [ &#34;$?&#34; -ne &#34;0&#34; ]\n    do\n
  \       echo &#34;failed to apply /srv/$manifest, retrying in 5 sec&#34;\n        sleep
  5s\n    done\ndone\necho &#34;kube-proxy successfully installed&#34;\n\n# restart
  ds to apply config from configmap\n/usr/bin/docker run -e KUBECONFIG=${KUBECONFIG}
  --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL delete pods -l k8s-app=kube-proxy
  -n kube-system\n\n# apply calico\nCALICO_FILE=&#34;calico-all.yaml&#34;\n\nwhile\n
  \   /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv
  -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$CALICO_FILE\n    [ &#34;$?&#34;
  -ne &#34;0&#34; ]\ndo\n    echo &#34;failed to apply /srv/$manifest, retrying in
  5 sec&#34;\n    sleep 5s\ndone\n\n# wait for healthy calico - we check for pods
  - desired vs ready\nwhile\n    # result of this is &#39;eval [ &#34;$DESIRED_POD_COUNT&#34;
  -eq &#34;$READY_POD_COUNT&#34; ]&#39;\n    /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG}
  --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL -n kube-system  get
  ds calico-node 2&gt;/dev/null &gt;/dev/null\n    RET_CODE_1=$?\n    eval $(/usr/bin/docker
  run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL -n kube-system get ds calico-node | tail -1 | awk &#39;{print &#34;[ \\&#34;&#34;
  $2&#34;\\&#34; -eq \\&#34;&#34;$4&#34;\\&#34; ] &#34;}&#39;)\n    RET_CODE_2=$?\n
  \   [ &#34;$RET_CODE_1&#34; -ne &#34;0&#34; ] || [ &#34;$RET_CODE_2&#34; -ne &#34;0&#34;
  ]\ndo\n    echo &#34;Waiting for calico to be ready . . &#34;\n    sleep 3s\ndone\n\n#
  apply default storage class\nif [ -f /srv/default-storage-class.yaml ]; then\n    while\n
  \       /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv
  -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/default-storage-class.yaml\n
  \       [ &#34;$?&#34; -ne &#34;0&#34; ]\n    do\n        echo &#34;failed to apply
  /srv/default-storage-class.yaml, retrying in 5 sec&#34;\n        sleep 5s\n    done\nelse\n
  \   echo &#34;no default storage class to apply&#34;\nfi\n\n# apply priority classes:\nPRIORITY_CLASSES_FILE=&#34;priority_classes.yaml&#34;\n\nwhile\n
  \   /usr/bin/docker run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv
  -v /etc/kubernetes:/etc/kubernetes $KUBECTL apply -f /srv/$PRIORITY_CLASSES_FILE\n
  \   [ &#34;$?&#34; -ne &#34;0&#34; ]\ndo\n    echo &#34;failed to apply /srv/$PRIORITY_CLASSES_FILE,
  retrying in 5 sec&#34;\n    sleep 5s\ndone\n\n# apply network policies:\nNETWORK_POLICIES_FILE=&#34;network_policies.yaml&#34;\nNAMESPACES=&#34;kube-system
  giantswarm&#34;\nfor namespace in ${NAMESPACES}; do\n    while\n      /usr/bin/docker
  run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL apply -f /srv/$NETWORK_POLICIES_FILE -n $namespace\n      [ &#34;$?&#34;
  -ne &#34;0&#34; ]\n    do\n      echo &#34;failed to apply /srv/$NETWORK_POLICIES_FILE,
  retrying in 5 sec&#34;\n      sleep 5s\n    done\ndone\n\n# apply k8s addons\nMANIFESTS=&#34;&#34;\nMANIFESTS=&#34;${MANIFESTS}
  ingress-controller-svc.yaml&#34;\nfor manifest in $MANIFESTS\ndo\n    while\n        /usr/bin/docker
  run -e KUBECONFIG=${KUBECONFIG} --net=host --rm -v /srv:/srv -v /etc/kubernetes:/etc/kubernetes
  $KUBECTL apply -f /srv/$manifest\n        [ &#34;$?&#34; -ne &#34;0&#34; ]\n    do\n
  \       echo &#34;failed to apply /srv/$manifest, retrying in 5 sec&#34;\n        sleep
  5s\n    done\ndone\necho &#34;Addons successfully installed&#34;\n\"\n\n    - path:
  /etc/kubernetes/kubeconfig/addons.yaml\n      filesystem: root\n      mode: 0644\n
  \     contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: proxy\n  user:\n    client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem\n
  \   client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n- name: local\n
  \ cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n    server:
  https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user: proxy\n
  \ name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/config/proxy-kubeconfig.yaml\n      filesystem: root\n
  \     mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: proxy\n  user:\n    client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem\n
  \   client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n- name: local\n
  \ cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n    server:
  https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user: proxy\n
  \ name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/kubeconfig/kube-proxy.yaml\n      filesystem: root\n
  \     mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: proxy\n  user:\n    client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem\n
  \   client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n- name: local\n
  \ cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n    server:
  https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user: proxy\n
  \ name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/config/kubelet.yaml.tmpl\n      filesystem: root\n      mode:
  0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,kind:
  KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\naddress: ${DEFAULT_IPV4}\nport:
  10250\nhealthzBindAddress: ${DEFAULT_IPV4}\nhealthzPort: 10248\nclusterDNS:\n  -
  K8sDNSIP\nclusterDomain: K8sDomain\nstaticPodPath: /etc/kubernetes/manifests\nevictionSoft:\n
  \ memory.available: &#34;500Mi&#34;\nevictionHard:\n  memory.available: &#34;200Mi&#34;\n
  \ imagefs.available: &#34;15%&#34;\nevictionSoftGracePeriod:\n  memory.available:
  &#34;5s&#34;\nevictionMaxPodGracePeriod: 60\nkubeReserved:\n  cpu: 350m\n  memory:
  1280Mi\n  ephemeral-storage: 1024Mi\nkubeReservedCgroup: /kubereserved.slice\nruntimeCgroups:
  /kubereserved.slice\nsystemReserved:\n  cpu: 250m\n  memory: 384Mi\nsystemReservedCgroup:
  /system.slice\nauthentication:\n  anonymous:\n    enabled: true # Defaults to false
  as of 1.10\n  webhook:\n    enabled: false # Deafults to true as of 1.10\nauthorization:\n
  \ mode: AlwaysAllow # Deafults to webhook as of 1.10\nfeatureGates:\n  TTLAfterFinished:
  true\n\"\n\n    - path: /etc/kubernetes/kubeconfig/kubelet.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: kubelet\n  user:\n    client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem\n
  \   client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n- name: local\n
  \ cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n    server:
  https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user: kubelet\n
  \ name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/kubeconfig/controller-manager.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: controller-manager\n  user:\n    client-certificate:
  /etc/kubernetes/ssl/apiserver-crt.pem\n    client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n-
  name: local\n  cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n
  \   server: https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user:
  controller-manager\n  name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/config/scheduler.yaml\n      filesystem: root\n      mode:
  0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,kind:
  KubeSchedulerConfiguration\nalgorithmSource:\n  provider: DefaultProvider\napiVersion:
  kubescheduler.config.k8s.io/v1alpha1\nclientConnection:\n  kubeconfig: /etc/kubernetes/kubeconfig/scheduler.yaml\nfailureDomains:
  kubernetes.io/hostname,failure-domain.beta.kubernetes.io/zone,failure-domain.beta.kubernetes.io/region\nhardPodAffinitySymmetricWeight:
  1\n\"\n\n    - path: /etc/kubernetes/kubeconfig/scheduler.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Config\nusers:\n- name: scheduler\n  user:\n    client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem\n
  \   client-key: /etc/kubernetes/ssl/apiserver-key.pem\nclusters:\n- name: local\n
  \ cluster:\n    certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem\n    server:
  https://APIDomain\ncontexts:\n- context:\n    cluster: local\n    user: scheduler\n
  \ name: service-account-context\ncurrent-context: service-account-context\n\"\n\n
  \   - path: /etc/kubernetes/encryption/k8s-encryption-config.yaml\n      filesystem:
  root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,kind:
  EncryptionConfiguration\napiVersion: apiserver.config.k8s.io/v1\nresources:\n  -
  resources:\n    - secrets\n    providers:\n    - aescbc:\n        keys:\n        -
  name: key1\n          secret: \"\n    - path: /etc/kubernetes/policies/audit-policy.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  audit.k8s.io/v1\nkind: Policy\nrules:\n  # The following requests were manually
  identified as high-volume and low-risk,\n  # so drop them.\n  - level: None\n    users:
  [&#34;system:kube-proxy&#34;]\n    verbs: [&#34;watch&#34;]\n    resources:\n      -
  group: &#34;&#34; # core\n        resources: [&#34;endpoints&#34;, &#34;services&#34;,
  &#34;services/status&#34;]\n  - level: None\n    # Ingress controller reads &#39;configmaps/ingress-uid&#39;
  through the unsecured port.\n    users: [&#34;system:unsecured&#34;]\n    namespaces:
  [&#34;kube-system&#34;]\n    verbs: [&#34;get&#34;]\n    resources:\n      - group:
  &#34;&#34; # core\n        resources: [&#34;configmaps&#34;]\n  - level: None\n
  \   users: [&#34;kubelet&#34;] # legacy kubelet identity\n    verbs: [&#34;get&#34;]\n
  \   resources:\n      - group: &#34;&#34; # core\n        resources: [&#34;nodes&#34;,
  &#34;nodes/status&#34;]\n  - level: None\n    userGroups: [&#34;system:nodes&#34;]\n
  \   verbs: [&#34;get&#34;]\n    resources:\n      - group: &#34;&#34; # core\n        resources:
  [&#34;nodes&#34;, &#34;nodes/status&#34;]\n  - level: None\n    users:\n      -
  system:kube-controller-manager\n      - system:kube-scheduler\n      - system:serviceaccount:kube-system:endpoint-controller\n
  \   verbs: [&#34;get&#34;, &#34;update&#34;]\n    namespaces: [&#34;kube-system&#34;]\n
  \   resources:\n      - group: &#34;&#34; # core\n        resources: [&#34;endpoints&#34;]\n
  \ - level: None\n    users: [&#34;system:apiserver&#34;]\n    verbs: [&#34;get&#34;]\n
  \   resources:\n      - group: &#34;&#34; # core\n        resources: [&#34;namespaces&#34;,
  &#34;namespaces/status&#34;, &#34;namespaces/finalize&#34;]\n  - level: None\n    users:
  [&#34;system:serviceaccount:kube-system:cluster-autoscaler&#34;]\n    verbs: [&#34;get&#34;,
  &#34;update&#34;]\n    namespaces: [&#34;kube-system&#34;]\n    resources:\n      -
  group: &#34;&#34; # core\n        resources: [&#34;configmaps&#34;, &#34;endpoints&#34;]\n
  \ # Don&#39;t log HPA fetching metrics.\n  - level: None\n    users:\n      - system:kube-controller-manager\n
  \   verbs: [&#34;get&#34;, &#34;list&#34;]\n    resources:\n      - group: &#34;metrics.k8s.io&#34;\n
  \ # Don&#39;t log these read-only URLs.\n  - level: None\n    nonResourceURLs:\n
  \     - /healthz*\n      - /version\n      - /swagger*\n  # Don&#39;t log events
  requests.\n  - level: None\n    resources:\n      - group: &#34;&#34; # core\n        resources:
  [&#34;events&#34;]\n  # node and pod status calls from nodes are high-volume and
  can be large, don&#39;t log responses for expected updates from nodes\n  - level:
  Request\n    users:\n      [\n        &#34;kubelet&#34;,\n        &#34;system:node-problem-detector&#34;,\n
  \       &#34;system:serviceaccount:kube-system:node-problem-detector&#34;,\n      ]\n
  \   verbs: [&#34;update&#34;, &#34;patch&#34;]\n    resources:\n      - group: &#34;&#34;
  # core\n        resources: [&#34;nodes/status&#34;, &#34;pods/status&#34;]\n    omitStages:\n
  \     - &#34;RequestReceived&#34;\n  - level: Request\n    userGroups: [&#34;system:nodes&#34;]\n
  \   verbs: [&#34;update&#34;, &#34;patch&#34;]\n    resources:\n      - group: &#34;&#34;
  # core\n        resources: [&#34;nodes/status&#34;, &#34;pods/status&#34;]\n    omitStages:\n
  \     - &#34;RequestReceived&#34;\n  # deletecollection calls can be large, don&#39;t
  log responses for expected namespace deletions\n  - level: Request\n    users: [&#34;system:serviceaccount:kube-system:namespace-controller&#34;]\n
  \   verbs: [&#34;deletecollection&#34;]\n    omitStages:\n      - &#34;RequestReceived&#34;\n
  \ # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,\n
  \ # so only log at the Metadata level.\n  - level: Metadata\n    resources:\n      -
  group: &#34;&#34; # core\n        resources: [&#34;secrets&#34;, &#34;configmaps&#34;]\n
  \     - group: authentication.k8s.io\n        resources: [&#34;tokenreviews&#34;]\n
  \   omitStages:\n      - &#34;RequestReceived&#34;\n  # Get repsonses can be large;
  skip them.\n  - level: Request\n    verbs: [&#34;get&#34;, &#34;list&#34;, &#34;watch&#34;]\n
  \   resources:\n      - group: &#34;&#34; # core\n      - group: &#34;admissionregistration.k8s.io&#34;\n
  \     - group: &#34;apiextensions.k8s.io&#34;\n      - group: &#34;apiregistration.k8s.io&#34;\n
  \     - group: &#34;apps&#34;\n      - group: &#34;authentication.k8s.io&#34;\n
  \     - group: &#34;authorization.k8s.io&#34;\n      - group: &#34;autoscaling&#34;\n
  \     - group: &#34;batch&#34;\n      - group: &#34;certificates.k8s.io&#34;\n      -
  group: &#34;extensions&#34;\n      - group: &#34;metrics.k8s.io&#34;\n      - group:
  &#34;networking.k8s.io&#34;\n      - group: &#34;policy&#34;\n      - group: &#34;rbac.authorization.k8s.io&#34;\n
  \     - group: &#34;scheduling.k8s.io&#34;\n      - group: &#34;settings.k8s.io&#34;\n
  \     - group: &#34;storage.k8s.io&#34;\n    omitStages:\n      - &#34;RequestReceived&#34;\n
  \ # Default level for known APIs\n  - level: RequestResponse\n    resources:\n      -
  group: &#34;&#34; # core\n      - group: &#34;admissionregistration.k8s.io&#34;\n
  \     - group: &#34;apiextensions.k8s.io&#34;\n      - group: &#34;apiregistration.k8s.io&#34;\n
  \     - group: &#34;apps&#34;\n      - group: &#34;authentication.k8s.io&#34;\n
  \     - group: &#34;authorization.k8s.io&#34;\n      - group: &#34;autoscaling&#34;\n
  \     - group: &#34;batch&#34;\n      - group: &#34;certificates.k8s.io&#34;\n      -
  group: &#34;extensions&#34;\n      - group: &#34;metrics.k8s.io&#34;\n      - group:
  &#34;networking.k8s.io&#34;\n      - group: &#34;policy&#34;\n      - group: &#34;rbac.authorization.k8s.io&#34;\n
  \     - group: &#34;scheduling.k8s.io&#34;\n      - group: &#34;settings.k8s.io&#34;\n
  \     - group: &#34;storage.k8s.io&#34;\n    omitStages:\n      - &#34;RequestReceived&#34;\n
  \ # Default level for all other requests.\n  - level: Metadata\n    omitStages:\n
  \     - &#34;RequestReceived&#34;\n\"\n    - path: /etc/kubernetes/manifests/k8s-api-healthz.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Pod\nmetadata:\n  name: k8s-api-healthz\n  namespace: kube-system\n  annotations:\n
  \   scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;\nspec:\n  hostNetwork:
  true\n  priorityClassName: system-node-critical\n  containers:\n    - name: k8s-api-healthz\n
  \     env:\n      - name: HOST_IP\n        valueFrom:\n          fieldRef:\n            fieldPath:
  status.podIP\n      command:\n        - /k8s-api-healthz\n        - --api-endpoint=&#34;https://$(HOST_IP):443/healthz&#34;\n
  \     image: quay.io/giantswarm/k8s-api-healthz:1c0cdf1ed5ee18fdf59063ecdd84bf3787f80fac\n
  \     resources:\n        requests:\n          cpu: 50m\n          memory: 20Mi\n
  \     volumeMounts:\n      - mountPath: /etc/kubernetes/ssl/\n        name: ssl-certs-kubernetes\n
  \       readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/ssl\n
  \   name: ssl-certs-kubernetes\n\"\n\n    - path: /etc/kubernetes/manifests/k8s-api-server.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Pod\nmetadata:\n  name: k8s-api-server\n  namespace: kube-system\n  labels:\n
  \   k8s-app: api-server\n    tier: control-plane\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod:
  &#39;&#39;\nspec:\n  hostNetwork: true\n  priorityClassName: system-node-critical\n
  \ containers:\n  - name: k8s-api-server\n    image: RegistryDomain/K8sImage\n    env:\n
  \   - name: HOST_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n
  \   command:\n    - /hyperkube\n    - kube-apiserver\n    - --allow-privileged=true\n
  \   - --anonymous-auth=false\n    - --insecure-port=0\n    - --kubelet-https=true\n
  \   - --kubelet-preferred-address-types=InternalIP\n    - --secure-port=9001\n    -
  --bind-address=0.0.0.0\n    - --etcd-prefix=EtcdPrefix\n    - --profiling=false\n
  \   - --service-account-lookup=true\n    - --authorization-mode=RBAC\n    - --feature-gates=TTLAfterFinished=true\n
  \   - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,DefaultStorageClass,PersistentVolumeClaimResize,PodSecurityPolicy,Priority,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook\n
  \   - --cloud-provider=\"\n\n    - path: /etc/kubernetes/manifests/k8s-controller-manager.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Pod\nmetadata:\n  name: k8s-controller-manager\n  namespace: kube-system\n
  \ labels:\n    k8s-app: controller-manager\n    tier: control-plane\n  annotations:\n
  \   scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;\nspec:\n  hostNetwork:
  true\n  priorityClassName: system-node-critical\n  containers:\n  - name: k8s-controller-manager\n
  \   image: RegistryDomain/K8sImage\n    command:\n    - /hyperkube\n    - kube-controller-manager\n
  \   - --logtostderr=true\n    - --v=2\n    - --cloud-provider=aws\n    - --terminated-pod-gc-threshold=10\n
  \   - --use-service-account-credentials=true\n    - --kubeconfig=/etc/kubernetes/kubeconfig/controller-manager.yaml\n
  \   - --feature-gates=TTLAfterFinished=true\n    - --root-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem\n
  \   - --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem\n
  \   resources:\n      requests:\n        cpu: 200m\n        memory: 200Mi\n    livenessProbe:\n
  \     httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 10251\n
  \     initialDelaySeconds: 15\n      timeoutSeconds: 15\n    volumeMounts:\n    -
  mountPath: /etc/kubernetes/config/\n      name: k8s-config\n      readOnly: true\n
  \   - mountPath: /etc/kubernetes/kubeconfig/\n      name: k8s-kubeconfig\n      readOnly:
  true\n    - mountPath: /etc/kubernetes/secrets/\n      name: k8s-secrets\n      readOnly:
  true\n    - mountPath: /etc/kubernetes/ssl/\n      name: ssl-certs-kubernetes\n
  \     readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/config\n
  \   name: k8s-config\n  - hostPath:\n      path: /etc/kubernetes/kubeconfig\n    name:
  k8s-kubeconfig\n  - hostPath:\n      path: /etc/kubernetes/secrets\n    name: k8s-secrets\n
  \ - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n\"\n\n
  \   - path: /etc/kubernetes/manifests/k8s-scheduler.yaml\n      filesystem: root\n
  \     mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,apiVersion:
  v1\nkind: Pod\nmetadata:\n  name: k8s-scheduler\n  namespace: kube-system\n  labels:\n
  \   k8s-app: scheduler\n    tier: control-plane\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod:
  &#39;&#39;\nspec:\n  hostNetwork: true\n  priorityClassName: system-node-critical\n
  \ containers:\n  - name: k8s-scheduler\n    image: RegistryDomain/K8sImage\n    command:\n
  \   - /hyperkube\n    - kube-scheduler\n    - --feature-gates=TTLAfterFinished=true\n
  \   - --config=/etc/kubernetes/config/scheduler.yaml\n    - --v=2\n    resources:\n
  \     requests:\n        cpu: 100m\n        memory: 100Mi\n    livenessProbe:\n
  \     httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 10251\n
  \     initialDelaySeconds: 15\n      timeoutSeconds: 15\n    volumeMounts:\n    -
  mountPath: /etc/kubernetes/config/\n      name: k8s-config\n      readOnly: true\n
  \   - mountPath: /etc/kubernetes/kubeconfig/\n      name: k8s-kubeconfig\n      readOnly:
  true\n    - mountPath: /etc/kubernetes/ssl/\n      name: ssl-certs-kubernetes\n
  \     readOnly: true\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/config\n
  \   name: k8s-config\n  - hostPath:\n      path: /etc/kubernetes/kubeconfig\n    name:
  k8s-kubeconfig\n  - hostPath:\n      path: /etc/kubernetes/ssl\n    name: ssl-certs-kubernetes\n\"\n\n
  \   - path: /etc/ssh/sshd_config\n      filesystem: root\n      mode: 0644\n      contents:\n
  \       source: \"data:text/plain;charset=utf-8;base64,# Use most defaults for sshd
  configuration.\nSubsystem sftp internal-sftp\nClientAliveInterval 180\nUseDNS no\nUsePAM
  yes\nPrintLastLog no # handled by PAM\nPrintMotd no # handled by PAM\n# Non defaults
  (#100)\nClientAliveCountMax 2\nPasswordAuthentication no\nTrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem\nMaxAuthTries
  5\nLoginGraceTime 60\nAllowTcpForwarding no\nAllowAgentForwarding no\n\"\n\n    -
  path: /etc/sysctl.d/hardening.conf\n      filesystem: root\n      mode: 0600\n      contents:\n
  \       source: \"data:text/plain;charset=utf-8;base64,fs.inotify.max_user_watches
  = 16384\n# Default is 128, doubling for nodes with many pods\n# See https://github.com/giantswarm/giantswarm/issues/7711\nfs.inotify.max_user_instances
  = 8192\nkernel.kptr_restrict = 2\nkernel.sysrq = 0\nnet.ipv4.conf.all.log_martians
  = 1\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.accept_redirects
  = 0\nnet.ipv4.conf.default.log_martians = 1\nnet.ipv4.tcp_timestamps = 0\nnet.ipv6.conf.all.accept_redirects
  = 0\nnet.ipv6.conf.default.accept_redirects = 0\n# Increased mmapfs because some
  applications, like ES, need higher limit to store data properly\nvm.max_map_count
  = 262144\n# Ingress controller performance improvements\n# See https://github.com/kubernetes/ingress-nginx/issues/1939\nnet.core.somaxconn=32768\nnet.ipv4.ip_local_port_range=1024
  65535\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.all.arp_ignore = 1\nnet.ipv4.conf.all.arp_announce
  = 2\n\"\n\n    - path: /etc/audit/rules.d/10-docker.rules\n      filesystem: root\n
  \     mode: 0600\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,-w
  /usr/bin/docker -k docker\n-w /var/lib/docker -k docker\n-w /etc/docker -k docker\n-w
  /etc/systemd/system/docker.service.d/10-giantswarm-extra-args.conf -k docker\n-w
  /etc/systemd/system/docker.service.d/01-wait-docker.conf -k docker\n-w /usr/lib/systemd/system/docker.service
  -k docker\n-w /usr/lib/systemd/system/docker.socket -k docker\n\"\n\n    - path:
  /etc/modules-load.d/ip_vs.conf\n      filesystem: root\n      mode: 0600\n      contents:\n
  \       source: \"data:text/plain;charset=utf-8;base64,ip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack_ipv4\n\"\n\n
  \   - path: /opt/install-debug-tools\n      filesystem: root\n      mode: 0544\n
  \     contents:\n        source: \"data:text/plain;charset=utf-8;base64,#!/bin/bash\nset
  -eu\n\nmkdir -p /opt/bin\n\n# download calicoctl\nCALICOCTL_VERSION=v3.9.1\nwget
  https://github.com/projectcalico/calicoctl/releases/download/${CALICOCTL_VERSION}/calicoctl-linux-amd64\nmv
  calicoctl-linux-amd64 /opt/bin/calicoctl\nchmod &#43;x /opt/bin/calicoctl\n\n# download
  crictl\nCRICTL_VERSION=v1.15.0\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz\ntar
  xvf crictl-${CRICTL_VERSION}-linux-amd64.tar.gz\nmv crictl /opt/bin/crictl\nchmod
  &#43;x /opt/bin/crictl\nrm crictl-${CRICTL_VERSION}-linux-amd64.tar.gz\n\"\n\n    -
  path: /etc/calico/calicoctl.cfg\n      filesystem: root\n      mode: 0644\n      contents:\n
  \       source: \"data:text/plain;charset=utf-8;base64,apiVersion: projectcalico.org/v3\nkind:
  CalicoAPIConfig\nmetadata:\nspec:\n  etcdEndpoints: https://EtcdDomain:1234\n  etcdKeyFile:
  /etc/kubernetes/ssl/etcd/server-key.pem\n  etcdCertFile: /etc/kubernetes/ssl/etcd/server-crt.pem\n
  \ etcdCACertFile: /etc/kubernetes/ssl/etcd/server-ca.pem\n\"\n\n    - path: /etc/crictl.yaml\n
  \     filesystem: root\n      mode: 0644\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,runtime-endpoint:
  unix:///var/run/dockershim/dockershim.sock\nimage-endpoint: unix:///var/run/dockershim/dockershim.sock\ntimeout:
  10\ndebug: false\n\"\n\n    - path: /etc/profile.d/setup-etcdctl.sh\n      filesystem:
  root\n      mode: 0444\n      contents:\n        source: \"data:text/plain;charset=utf-8;base64,alias
  etcdctl=&#34;ETCDCTL_API=3 \\\n    ETCDCTL_ENDPOINTS=https://EtcdDomain:1234 \\\n
  \   ETCDCTL_CACERT=/etc/kubernetes/ssl/etcd/client-ca.pem \\\n    ETCDCTL_CERT=/etc/kubernetes/ssl/etcd/client-crt.pem
  \\\n    ETCDCTL_KEY=/etc/kubernetes/ssl/etcd/client-key.pem \\\n    etcdctl&#34;\n\"\n"
